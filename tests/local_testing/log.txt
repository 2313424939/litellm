============================= test session starts ==============================
platform darwin -- Python 3.11.4, pytest-7.4.1, pluggy-1.2.0
rootdir: /Users/krrishdholakia/Documents/litellm/tests/local_testing
plugins: snapshot-0.9.0, cov-5.0.0, timeout-2.2.0, respx-0.21.1, asyncio-0.21.1, mock-3.11.1, Faker-25.9.2, anyio-4.2.0
asyncio: mode=Mode.STRICT
collected 1 item

test_completion.py F                                                     [100%]

=================================== FAILURES ===================================
________________________ test_completion_watsonx_error _________________________

model = 'ibm/granite-13b-chat-v2'
messages = [{'content': 'Write a short poem about the sky', 'role': 'user'}]
timeout = 600.0, temperature = None, top_p = None, n = None, stream = None
stream_options = None, stop = ['stop'], max_completion_tokens = None
max_tokens = 20, modalities = None, prediction = None, audio = None
presence_penalty = None, frequency_penalty = None, logit_bias = None
user = None, response_format = None, seed = None, tools = None
tool_choice = None, logprobs = None, top_logprobs = None
parallel_tool_calls = None, deployment_id = None, extra_headers = None
functions = None, function_call = None, base_url = None, api_version = None
api_key = 'hGBBF6nx-tJRzXcN_NereGOPb_J8lyguQc5cceQeRBec', model_list = None
kwargs = {'litellm_call_id': 'df6d0a8e-daf4-49a5-9b55-5237c36d0251', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x124477750>}
args = {'acompletion': False, 'api_base': 'https://us-south.ml.cloud.ibm.com', 'api_key': 'hGBBF6nx-tJRzXcN_NereGOPb_J8lyguQc5cceQeRBec', 'api_version': None, ...}
api_base = 'https://us-south.ml.cloud.ibm.com', mock_response = None
mock_tool_calls = None, force_timeout = 600, logger_fn = None, verbose = False

    @client
    def completion(  # type: ignore # noqa: PLR0915
        model: str,
        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create
        messages: List = [],
        timeout: Optional[Union[float, str, httpx.Timeout]] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        n: Optional[int] = None,
        stream: Optional[bool] = None,
        stream_options: Optional[dict] = None,
        stop=None,
        max_completion_tokens: Optional[int] = None,
        max_tokens: Optional[int] = None,
        modalities: Optional[List[ChatCompletionModality]] = None,
        prediction: Optional[ChatCompletionPredictionContentParam] = None,
        audio: Optional[ChatCompletionAudioParam] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[dict] = None,
        user: Optional[str] = None,
        # openai v1.0+ new params
        response_format: Optional[Union[dict, Type[BaseModel]]] = None,
        seed: Optional[int] = None,
        tools: Optional[List] = None,
        tool_choice: Optional[Union[str, dict]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        parallel_tool_calls: Optional[bool] = None,
        deployment_id=None,
        extra_headers: Optional[dict] = None,
        # soon to be deprecated params by OpenAI
        functions: Optional[List] = None,
        function_call: Optional[str] = None,
        # set api_base, api_version, api_key
        base_url: Optional[str] = None,
        api_version: Optional[str] = None,
        api_key: Optional[str] = None,
        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.
        # Optional liteLLM function params
        **kwargs,
    ) -> Union[ModelResponse, CustomStreamWrapper]:
        """
        Perform a completion() using any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)
        Parameters:
            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/
            messages (List): A list of message objects representing the conversation context (default is an empty list).
    
            OPTIONAL PARAMS
            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).
            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).
            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).
            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).
            n (int, optional): The number of completions to generate (default is 1).
            stream (bool, optional): If True, return a streaming response (default is False).
            stream_options (dict, optional): A dictionary containing options for the streaming response. Only set this when you set stream: true.
            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.
            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).
            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request.. You can use `["text", "audio"]`
            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.
            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: ["audio"]
            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.
            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.
            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.
            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.
            logprobs (bool, optional): Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message
            top_logprobs (int, optional): An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.
            api_base (str, optional): Base URL for the API (default is None).
            api_version (str, optional): API version (default is None).
            api_key (str, optional): API key (default is None).
            model_list (list, optional): List of api base, version, keys
            extra_headers (dict, optional): Additional headers to include in the request.
    
            LITELLM Specific Params
            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).
            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model="amazon.titan-tg1-large" and custom_llm_provider="bedrock"
            max_retries (int, optional): The number of retries to attempt (default is 0).
        Returns:
            ModelResponse: A response object containing the generated completion and associated metadata.
    
        Note:
            - This function is used to perform completions() using the specified language model.
            - It supports various optional parameters for customizing the completion behavior.
            - If 'mock_response' is provided, a mock completion response is returned for testing or debugging.
        """
        ######### unpacking kwargs #####################
        args = locals()
        api_base = kwargs.get("api_base", None)
        mock_response = kwargs.get("mock_response", None)
        mock_tool_calls = kwargs.get("mock_tool_calls", None)
        force_timeout = kwargs.get("force_timeout", 600)  ## deprecated
        logger_fn = kwargs.get("logger_fn", None)
        verbose = kwargs.get("verbose", False)
        custom_llm_provider = kwargs.get("custom_llm_provider", None)
        litellm_logging_obj = kwargs.get("litellm_logging_obj", None)
        id = kwargs.get("id", None)
        metadata = kwargs.get("metadata", None)
        model_info = kwargs.get("model_info", None)
        proxy_server_request = kwargs.get("proxy_server_request", None)
        fallbacks = kwargs.get("fallbacks", None)
        headers = kwargs.get("headers", None) or extra_headers
        ensure_alternating_roles: Optional[bool] = kwargs.get(
            "ensure_alternating_roles", None
        )
        user_continue_message: Optional[ChatCompletionUserMessage] = kwargs.get(
            "user_continue_message", None
        )
        assistant_continue_message: Optional[ChatCompletionAssistantMessage] = kwargs.get(
            "assistant_continue_message", None
        )
        if headers is None:
            headers = {}
    
        if extra_headers is not None:
            headers.update(extra_headers)
        num_retries = kwargs.get(
            "num_retries", None
        )  ## alt. param for 'max_retries'. Use this to pass retries w/ instructor.
        max_retries = kwargs.get("max_retries", None)
        cooldown_time = kwargs.get("cooldown_time", None)
        context_window_fallback_dict = kwargs.get("context_window_fallback_dict", None)
        organization = kwargs.get("organization", None)
        ### CUSTOM MODEL COST ###
        input_cost_per_token = kwargs.get("input_cost_per_token", None)
        output_cost_per_token = kwargs.get("output_cost_per_token", None)
        input_cost_per_second = kwargs.get("input_cost_per_second", None)
        output_cost_per_second = kwargs.get("output_cost_per_second", None)
        ### CUSTOM PROMPT TEMPLATE ###
        initial_prompt_value = kwargs.get("initial_prompt_value", None)
        roles = kwargs.get("roles", None)
        final_prompt_value = kwargs.get("final_prompt_value", None)
        bos_token = kwargs.get("bos_token", None)
        eos_token = kwargs.get("eos_token", None)
        preset_cache_key = kwargs.get("preset_cache_key", None)
        hf_model_name = kwargs.get("hf_model_name", None)
        supports_system_message = kwargs.get("supports_system_message", None)
        base_model = kwargs.get("base_model", None)
        ### TEXT COMPLETION CALLS ###
        text_completion = kwargs.get("text_completion", False)
        atext_completion = kwargs.get("atext_completion", False)
        ### ASYNC CALLS ###
        acompletion = kwargs.get("acompletion", False)
        client = kwargs.get("client", None)
        ### Admin Controls ###
        no_log = kwargs.get("no-log", False)
        ### PROMPT MANAGEMENT ###
        prompt_id = cast(Optional[str], kwargs.get("prompt_id", None))
        prompt_variables = cast(Optional[dict], kwargs.get("prompt_variables", None))
        ### COPY MESSAGES ### - related issue https://github.com/BerriAI/litellm/discussions/4489
        messages = get_completion_messages(
            messages=messages,
            ensure_alternating_roles=ensure_alternating_roles or False,
            user_continue_message=user_continue_message,
            assistant_continue_message=assistant_continue_message,
        )
        ######## end of unpacking kwargs ###########
        openai_params = [
            "functions",
            "function_call",
            "temperature",
            "temperature",
            "top_p",
            "n",
            "stream",
            "stream_options",
            "stop",
            "max_completion_tokens",
            "modalities",
            "prediction",
            "audio",
            "max_tokens",
            "presence_penalty",
            "frequency_penalty",
            "logit_bias",
            "user",
            "request_timeout",
            "api_base",
            "api_version",
            "api_key",
            "deployment_id",
            "organization",
            "base_url",
            "default_headers",
            "timeout",
            "response_format",
            "seed",
            "tools",
            "tool_choice",
            "max_retries",
            "parallel_tool_calls",
            "logprobs",
            "top_logprobs",
            "extra_headers",
        ]
    
        default_params = openai_params + all_litellm_params
    
        litellm_params = {}  # used to prevent unbound var errors
        non_default_params = {
            k: v for k, v in kwargs.items() if k not in default_params
        }  # model-specific params - pass them straight to the model/provider
    
        ## PROMPT MANAGEMENT HOOKS ##
    
        if isinstance(litellm_logging_obj, LiteLLMLoggingObj) and prompt_id is not None:
            model, messages, optional_params = (
                litellm_logging_obj.get_chat_completion_prompt(
                    model=model,
                    messages=messages,
                    non_default_params=non_default_params,
                    headers=headers,
                    prompt_id=prompt_id,
                    prompt_variables=prompt_variables,
                )
            )
    
        try:
            if base_url is not None:
                api_base = base_url
            if num_retries is not None:
                max_retries = num_retries
            logging = litellm_logging_obj
            fallbacks = fallbacks or litellm.model_fallbacks
            if fallbacks is not None:
                return completion_with_fallbacks(**args)
            if model_list is not None:
                deployments = [
                    m["litellm_params"] for m in model_list if m["model_name"] == model
                ]
                return litellm.batch_completion_models(deployments=deployments, **args)
            if litellm.model_alias_map and model in litellm.model_alias_map:
                model = litellm.model_alias_map[
                    model
                ]  # update the model to the actual value if an alias has been passed in
            model_response = ModelResponse()
            setattr(model_response, "usage", litellm.Usage())
            if (
                kwargs.get("azure", False) is True
            ):  # don't remove flag check, to remain backwards compatible for repos like Codium
                custom_llm_provider = "azure"
            if deployment_id is not None:  # azure llms
                model = deployment_id
                custom_llm_provider = "azure"
            model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                model=model,
                custom_llm_provider=custom_llm_provider,
                api_base=api_base,
                api_key=api_key,
            )
            if model_response is not None and hasattr(model_response, "_hidden_params"):
                model_response._hidden_params["custom_llm_provider"] = custom_llm_provider
                model_response._hidden_params["region_name"] = kwargs.get(
                    "aws_region_name", None
                )  # support region-based pricing for bedrock
    
            ### VALIDATE USER MESSAGES ###
            messages = validate_chat_completion_messages(messages=messages)
    
            ### TIMEOUT LOGIC ###
            timeout = timeout or kwargs.get("request_timeout", 600) or 600
            # set timeout for 10 minutes by default
            if isinstance(timeout, httpx.Timeout) and not supports_httpx_timeout(
                custom_llm_provider
            ):
                timeout = timeout.read or 600  # default 10 min timeout
            elif not isinstance(timeout, httpx.Timeout):
                timeout = float(timeout)  # type: ignore
    
            ### REGISTER CUSTOM MODEL PRICING -- IF GIVEN ###
            if input_cost_per_token is not None and output_cost_per_token is not None:
                litellm.register_model(
                    {
                        f"{custom_llm_provider}/{model}": {
                            "input_cost_per_token": input_cost_per_token,
                            "output_cost_per_token": output_cost_per_token,
                            "litellm_provider": custom_llm_provider,
                        }
                    }
                )
            elif (
                input_cost_per_second is not None
            ):  # time based pricing just needs cost in place
                output_cost_per_second = output_cost_per_second
                litellm.register_model(
                    {
                        f"{custom_llm_provider}/{model}": {
                            "input_cost_per_second": input_cost_per_second,
                            "output_cost_per_second": output_cost_per_second,
                            "litellm_provider": custom_llm_provider,
                        }
                    }
                )
            ### BUILD CUSTOM PROMPT TEMPLATE -- IF GIVEN ###
            custom_prompt_dict = {}  # type: ignore
            if (
                initial_prompt_value
                or roles
                or final_prompt_value
                or bos_token
                or eos_token
            ):
                custom_prompt_dict = {model: {}}
                if initial_prompt_value:
                    custom_prompt_dict[model]["initial_prompt_value"] = initial_prompt_value
                if roles:
                    custom_prompt_dict[model]["roles"] = roles
                if final_prompt_value:
                    custom_prompt_dict[model]["final_prompt_value"] = final_prompt_value
                if bos_token:
                    custom_prompt_dict[model]["bos_token"] = bos_token
                if eos_token:
                    custom_prompt_dict[model]["eos_token"] = eos_token
    
            if (
                supports_system_message is not None
                and isinstance(supports_system_message, bool)
                and supports_system_message is False
            ):
                messages = map_system_message_pt(messages=messages)
    
            if dynamic_api_key is not None:
                api_key = dynamic_api_key
            # check if user passed in any of the OpenAI optional params
            optional_params = get_optional_params(
                functions=functions,
                function_call=function_call,
                temperature=temperature,
                top_p=top_p,
                n=n,
                stream=stream,
                stream_options=stream_options,
                stop=stop,
                max_tokens=max_tokens,
                max_completion_tokens=max_completion_tokens,
                modalities=modalities,
                prediction=prediction,
                audio=audio,
                presence_penalty=presence_penalty,
                frequency_penalty=frequency_penalty,
                logit_bias=logit_bias,
                user=user,
                # params to identify the model
                model=model,
                custom_llm_provider=custom_llm_provider,
                response_format=response_format,
                seed=seed,
                tools=tools,
                tool_choice=tool_choice,
                max_retries=max_retries,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                api_version=api_version,
                parallel_tool_calls=parallel_tool_calls,
                messages=messages,
                **non_default_params,
            )
    
            if litellm.add_function_to_prompt and optional_params.get(
                "functions_unsupported_model", None
            ):  # if user opts to add it to prompt, when API doesn't support function calling
                functions_unsupported_model = optional_params.pop(
                    "functions_unsupported_model"
                )
                messages = function_call_prompt(
                    messages=messages, functions=functions_unsupported_model
                )
    
            # For logging - save the values of the litellm-specific params passed in
            litellm_params = get_litellm_params(
                acompletion=acompletion,
                api_key=api_key,
                force_timeout=force_timeout,
                logger_fn=logger_fn,
                verbose=verbose,
                custom_llm_provider=custom_llm_provider,
                api_base=api_base,
                litellm_call_id=kwargs.get("litellm_call_id", None),
                model_alias_map=litellm.model_alias_map,
                completion_call_id=id,
                metadata=metadata,
                model_info=model_info,
                proxy_server_request=proxy_server_request,
                preset_cache_key=preset_cache_key,
                no_log=no_log,
                input_cost_per_second=input_cost_per_second,
                input_cost_per_token=input_cost_per_token,
                output_cost_per_second=output_cost_per_second,
                output_cost_per_token=output_cost_per_token,
                cooldown_time=cooldown_time,
                text_completion=kwargs.get("text_completion"),
                azure_ad_token_provider=kwargs.get("azure_ad_token_provider"),
                user_continue_message=kwargs.get("user_continue_message"),
                base_model=base_model,
                litellm_trace_id=kwargs.get("litellm_trace_id"),
                hf_model_name=hf_model_name,
                custom_prompt_dict=custom_prompt_dict,
            )
            logging.update_environment_variables(
                model=model,
                user=user,
                optional_params=optional_params,
                litellm_params=litellm_params,
                custom_llm_provider=custom_llm_provider,
            )
            if mock_response or mock_tool_calls:
                return mock_completion(
                    model,
                    messages,
                    stream=stream,
                    n=n,
                    mock_response=mock_response,
                    mock_tool_calls=mock_tool_calls,
                    logging=logging,
                    acompletion=acompletion,
                    mock_delay=kwargs.get("mock_delay", None),
                    custom_llm_provider=custom_llm_provider,
                )
    
            if custom_llm_provider == "azure":
                # azure configs
                ## check dynamic params ##
                dynamic_params = False
                if client is not None and (
                    isinstance(client, openai.AzureOpenAI)
                    or isinstance(client, openai.AsyncAzureOpenAI)
                ):
                    dynamic_params = _check_dynamic_azure_params(
                        azure_client_params={"api_version": api_version},
                        azure_client=client,
                    )
    
                api_type = get_secret("AZURE_API_TYPE") or "azure"
    
                api_base = api_base or litellm.api_base or get_secret("AZURE_API_BASE")
    
                api_version = (
                    api_version
                    or litellm.api_version
                    or get_secret("AZURE_API_VERSION")
                    or litellm.AZURE_DEFAULT_API_VERSION
                )
    
                api_key = (
                    api_key
                    or litellm.api_key
                    or litellm.azure_key
                    or get_secret("AZURE_OPENAI_API_KEY")
                    or get_secret("AZURE_API_KEY")
                )
    
                azure_ad_token = optional_params.get("extra_body", {}).pop(
                    "azure_ad_token", None
                ) or get_secret("AZURE_AD_TOKEN")
    
                headers = headers or litellm.headers
    
                if extra_headers is not None:
                    optional_params["extra_headers"] = extra_headers
    
                if (
                    litellm.enable_preview_features
                    and litellm.AzureOpenAIO1Config().is_o1_model(model=model)
                ):
                    ## LOAD CONFIG - if set
                    config = litellm.AzureOpenAIO1Config.get_config()
                    for k, v in config.items():
                        if (
                            k not in optional_params
                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in
                            optional_params[k] = v
    
                    response = azure_o1_chat_completions.completion(
                        model=model,
                        messages=messages,
                        headers=headers,
                        api_key=api_key,
                        api_base=api_base,
                        api_version=api_version,
                        api_type=api_type,
                        dynamic_params=dynamic_params,
                        azure_ad_token=azure_ad_token,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        logging_obj=logging,
                        acompletion=acompletion,
                        timeout=timeout,  # type: ignore
                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client
                    )
                else:
                    ## LOAD CONFIG - if set
                    config = litellm.AzureOpenAIConfig.get_config()
                    for k, v in config.items():
                        if (
                            k not in optional_params
                        ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in
                            optional_params[k] = v
    
                    ## COMPLETION CALL
                    response = azure_chat_completions.completion(
                        model=model,
                        messages=messages,
                        headers=headers,
                        api_key=api_key,
                        api_base=api_base,
                        api_version=api_version,
                        api_type=api_type,
                        dynamic_params=dynamic_params,
                        azure_ad_token=azure_ad_token,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        logging_obj=logging,
                        acompletion=acompletion,
                        timeout=timeout,  # type: ignore
                        client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client
                    )
    
                if optional_params.get("stream", False):
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                        additional_args={
                            "headers": headers,
                            "api_version": api_version,
                            "api_base": api_base,
                        },
                    )
            elif custom_llm_provider == "azure_text":
                # azure configs
                api_type = get_secret("AZURE_API_TYPE") or "azure"
    
                api_base = api_base or litellm.api_base or get_secret("AZURE_API_BASE")
    
                api_version = (
                    api_version or litellm.api_version or get_secret("AZURE_API_VERSION")
                )
    
                api_key = (
                    api_key
                    or litellm.api_key
                    or litellm.azure_key
                    or get_secret("AZURE_OPENAI_API_KEY")
                    or get_secret("AZURE_API_KEY")
                )
    
                azure_ad_token = optional_params.get("extra_body", {}).pop(
                    "azure_ad_token", None
                ) or get_secret("AZURE_AD_TOKEN")
    
                headers = headers or litellm.headers
    
                if extra_headers is not None:
                    optional_params["extra_headers"] = extra_headers
    
                ## LOAD CONFIG - if set
                config = litellm.AzureOpenAIConfig.get_config()
                for k, v in config.items():
                    if (
                        k not in optional_params
                    ):  # completion(top_k=3) > azure_config(top_k=3) <- allows for dynamic variables to be passed in
                        optional_params[k] = v
    
                ## COMPLETION CALL
                response = azure_text_completions.completion(
                    model=model,
                    messages=messages,
                    headers=headers,
                    api_key=api_key,
                    api_base=api_base,
                    api_version=api_version,
                    api_type=api_type,
                    azure_ad_token=azure_ad_token,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    logging_obj=logging,
                    acompletion=acompletion,
                    timeout=timeout,
                    client=client,  # pass AsyncAzureOpenAI, AzureOpenAI client
                )
    
                if optional_params.get("stream", False) or acompletion is True:
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                        additional_args={
                            "headers": headers,
                            "api_version": api_version,
                            "api_base": api_base,
                        },
                    )
            elif custom_llm_provider == "azure_ai":
                api_base = (
                    api_base  # for deepinfra/perplexity/anyscale/groq/friendliai we check in get_llm_provider and pass in the api base from there
                    or litellm.api_base
                    or get_secret("AZURE_AI_API_BASE")
                )
                # set API KEY
                api_key = (
                    api_key
                    or litellm.api_key  # for deepinfra/perplexity/anyscale/friendliai we check in get_llm_provider and pass in the api key from there
                    or litellm.openai_key
                    or get_secret("AZURE_AI_API_KEY")
                )
    
                headers = headers or litellm.headers
    
                if extra_headers is not None:
                    optional_params["extra_headers"] = extra_headers
    
                ## LOAD CONFIG - if set
                config = litellm.AzureAIStudioConfig.get_config()
                for k, v in config.items():
                    if (
                        k not in optional_params
                    ):  # completion(top_k=3) > openai_config(top_k=3) <- allows for dynamic variables to be passed in
                        optional_params[k] = v
    
                ## FOR COHERE
                if "command-r" in model:  # make sure tool call in messages are str
                    messages = stringify_json_tool_call_content(messages=messages)
    
                ## COMPLETION CALL
                try:
                    response = openai_chat_completions.completion(
                        model=model,
                        messages=messages,
                        headers=headers,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        api_key=api_key,
                        api_base=api_base,
                        acompletion=acompletion,
                        logging_obj=logging,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        timeout=timeout,  # type: ignore
                        custom_prompt_dict=custom_prompt_dict,
                        client=client,  # pass AsyncOpenAI, OpenAI client
                        organization=organization,
                        custom_llm_provider=custom_llm_provider,
                        drop_params=non_default_params.get("drop_params"),
                    )
                except Exception as e:
                    ## LOGGING - log the original exception returned
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=str(e),
                        additional_args={"headers": headers},
                    )
                    raise e
    
                if optional_params.get("stream", False):
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                        additional_args={"headers": headers},
                    )
            elif (
                custom_llm_provider == "text-completion-openai"
                or "ft:babbage-002" in model
                or "ft:davinci-002" in model  # support for finetuned completion models
                or custom_llm_provider
                in litellm.openai_text_completion_compatible_providers
                and kwargs.get("text_completion") is True
            ):
                openai.api_type = "openai"
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("OPENAI_API_BASE")
                    or "https://api.openai.com/v1"
                )
    
                openai.api_version = None
                # set API KEY
    
                api_key = (
                    api_key
                    or litellm.api_key
                    or litellm.openai_key
                    or get_secret("OPENAI_API_KEY")
                )
    
                headers = headers or litellm.headers
    
                if extra_headers is not None:
                    optional_params["extra_headers"] = extra_headers
    
                ## LOAD CONFIG - if set
                config = litellm.OpenAITextCompletionConfig.get_config()
                for k, v in config.items():
                    if (
                        k not in optional_params
                    ):  # completion(top_k=3) > openai_text_config(top_k=3) <- allows for dynamic variables to be passed in
                        optional_params[k] = v
                if litellm.organization:
                    openai.organization = litellm.organization
    
                if (
                    len(messages) > 0
                    and "content" in messages[0]
                    and isinstance(messages[0]["content"], list)
                ):
                    # text-davinci-003 can accept a string or array, if it's an array, assume the array is set in messages[0]['content']
                    # https://platform.openai.com/docs/api-reference/completions/create
                    prompt = messages[0]["content"]
                else:
                    prompt = " ".join([message["content"] for message in messages])  # type: ignore
    
                ## COMPLETION CALL
                if custom_llm_provider == "together_ai":
                    _response = together_ai_text_completions.completion(
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        api_key=api_key,
                        api_base=api_base,
                        acompletion=acompletion,
                        client=client,  # pass AsyncOpenAI, OpenAI client
                        logging_obj=logging,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        timeout=timeout,  # type: ignore
                    )
                else:
                    _response = openai_text_completions.completion(
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        api_key=api_key,
                        api_base=api_base,
                        acompletion=acompletion,
                        client=client,  # pass AsyncOpenAI, OpenAI client
                        logging_obj=logging,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        timeout=timeout,  # type: ignore
                    )
    
                if (
                    optional_params.get("stream", False) is False
                    and acompletion is False
                    and text_completion is False
                ):
                    # convert to chat completion response
                    _response = litellm.OpenAITextCompletionConfig().convert_to_chat_model_response_object(
                        response_object=_response, model_response_object=model_response
                    )
    
                if optional_params.get("stream", False) or acompletion is True:
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=_response,
                        additional_args={"headers": headers},
                    )
                response = _response
            elif custom_llm_provider == "groq":
                api_base = (
                    api_base  # for deepinfra/perplexity/anyscale/groq/friendliai we check in get_llm_provider and pass in the api base from there
                    or litellm.api_base
                    or get_secret("GROQ_API_BASE")
                    or "https://api.groq.com/openai/v1"
                )
    
                # set API KEY
                api_key = (
                    api_key
                    or litellm.api_key  # for deepinfra/perplexity/anyscale/friendliai we check in get_llm_provider and pass in the api key from there
                    or litellm.groq_key
                    or get_secret("GROQ_API_KEY")
                )
    
                headers = headers or litellm.headers
    
                ## LOAD CONFIG - if set
                config = litellm.GroqChatConfig.get_config()
                for k, v in config.items():
                    if (
                        k not in optional_params
                    ):  # completion(top_k=3) > openai_config(top_k=3) <- allows for dynamic variables to be passed in
                        optional_params[k] = v
    
                response = groq_chat_completions.completion(
                    model=model,
                    messages=messages,
                    headers=headers,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    api_key=api_key,
                    api_base=api_base,
                    acompletion=acompletion,
                    logging_obj=logging,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    timeout=timeout,  # type: ignore
                    custom_prompt_dict=custom_prompt_dict,
                    client=client,  # pass AsyncOpenAI, OpenAI client
                    custom_llm_provider=custom_llm_provider,
                    encoding=encoding,
                )
            elif (
                model in litellm.open_ai_chat_completion_models
                or custom_llm_provider == "custom_openai"
                or custom_llm_provider == "deepinfra"
                or custom_llm_provider == "perplexity"
                or custom_llm_provider == "nvidia_nim"
                or custom_llm_provider == "cerebras"
                or custom_llm_provider == "sambanova"
                or custom_llm_provider == "volcengine"
                or custom_llm_provider == "deepseek"
                or custom_llm_provider == "anyscale"
                or custom_llm_provider == "mistral"
                or custom_llm_provider == "openai"
                or custom_llm_provider == "together_ai"
                or custom_llm_provider in litellm.openai_compatible_providers
                or "ft:gpt-3.5-turbo" in model  # finetune gpt-3.5-turbo
            ):  # allow user to make an openai call with a custom base
                # note: if a user sets a custom base - we should ensure this works
                # allow for the setting of dynamic and stateful api-bases
                api_base = (
                    api_base  # for deepinfra/perplexity/anyscale/groq/friendliai we check in get_llm_provider and pass in the api base from there
                    or litellm.api_base
                    or get_secret("OPENAI_API_BASE")
                    or "https://api.openai.com/v1"
                )
                organization = (
                    organization
                    or litellm.organization
                    or get_secret("OPENAI_ORGANIZATION")
                    or None  # default - https://github.com/openai/openai-python/blob/284c1799070c723c6a553337134148a7ab088dd8/openai/util.py#L105
                )
                openai.organization = organization
                # set API KEY
                api_key = (
                    api_key
                    or litellm.api_key  # for deepinfra/perplexity/anyscale/friendliai we check in get_llm_provider and pass in the api key from there
                    or litellm.openai_key
                    or get_secret("OPENAI_API_KEY")
                )
    
                headers = headers or litellm.headers
    
                if extra_headers is not None:
                    optional_params["extra_headers"] = extra_headers
    
                ## LOAD CONFIG - if set
                config = litellm.OpenAIConfig.get_config()
                for k, v in config.items():
                    if (
                        k not in optional_params
                    ):  # completion(top_k=3) > openai_config(top_k=3) <- allows for dynamic variables to be passed in
                        optional_params[k] = v
    
                ## COMPLETION CALL
                try:
                    response = openai_chat_completions.completion(
                        model=model,
                        messages=messages,
                        headers=headers,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        api_key=api_key,
                        api_base=api_base,
                        acompletion=acompletion,
                        logging_obj=logging,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        timeout=timeout,  # type: ignore
                        custom_prompt_dict=custom_prompt_dict,
                        client=client,  # pass AsyncOpenAI, OpenAI client
                        organization=organization,
                        custom_llm_provider=custom_llm_provider,
                    )
                except Exception as e:
                    ## LOGGING - log the original exception returned
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=str(e),
                        additional_args={"headers": headers},
                    )
                    raise e
    
                if optional_params.get("stream", False):
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                        additional_args={"headers": headers},
                    )
    
            elif (
                "replicate" in model
                or custom_llm_provider == "replicate"
                or model in litellm.replicate_models
            ):
                # Setting the relevant API KEY for replicate, replicate defaults to using os.environ.get("REPLICATE_API_TOKEN")
                replicate_key = (
                    api_key
                    or litellm.replicate_key
                    or litellm.api_key
                    or get_secret("REPLICATE_API_KEY")
                    or get_secret("REPLICATE_API_TOKEN")
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("REPLICATE_API_BASE")
                    or "https://api.replicate.com/v1"
                )
    
                custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
    
                model_response = replicate_chat_completion(  # type: ignore
                    model=model,
                    messages=messages,
                    api_base=api_base,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,  # for calculating input/output tokens
                    api_key=replicate_key,
                    logging_obj=logging,
                    custom_prompt_dict=custom_prompt_dict,
                    acompletion=acompletion,
                    headers=headers,
                )
    
                if optional_params.get("stream", False) is True:
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=replicate_key,
                        original_response=model_response,
                    )
    
                response = model_response
            elif (
                "clarifai" in model
                or custom_llm_provider == "clarifai"
                or model in litellm.clarifai_models
            ):
                clarifai_key = None
                clarifai_key = (
                    api_key
                    or litellm.clarifai_key
                    or litellm.api_key
                    or get_secret("CLARIFAI_API_KEY")
                    or get_secret("CLARIFAI_API_TOKEN")
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("CLARIFAI_API_BASE")
                    or "https://api.clarifai.com/v2"
                )
                api_base = litellm.ClarifaiConfig()._convert_model_to_url(model, api_base)
                response = base_llm_http_handler.completion(
                    model=model,
                    stream=stream,
                    fake_stream=True,  # clarifai does not support streaming, we fake it
                    messages=messages,
                    acompletion=acompletion,
                    api_base=api_base,
                    model_response=model_response,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_llm_provider="clarifai",
                    timeout=timeout,
                    headers=headers,
                    encoding=encoding,
                    api_key=clarifai_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                )
            elif custom_llm_provider == "anthropic_text":
                api_key = (
                    api_key
                    or litellm.anthropic_key
                    or litellm.api_key
                    or os.environ.get("ANTHROPIC_API_KEY")
                )
                custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("ANTHROPIC_API_BASE")
                    or get_secret("ANTHROPIC_BASE_URL")
                    or "https://api.anthropic.com/v1/complete"
                )
    
                if api_base is not None and not api_base.endswith("/v1/complete"):
                    api_base += "/v1/complete"
    
                response = base_llm_http_handler.completion(
                    model=model,
                    stream=stream,
                    messages=messages,
                    acompletion=acompletion,
                    api_base=api_base,
                    model_response=model_response,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_llm_provider="anthropic_text",
                    timeout=timeout,
                    headers=headers,
                    encoding=encoding,
                    api_key=api_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                )
            elif custom_llm_provider == "anthropic":
                api_key = (
                    api_key
                    or litellm.anthropic_key
                    or litellm.api_key
                    or os.environ.get("ANTHROPIC_API_KEY")
                )
                custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
                # call /messages
                # default route for all anthropic models
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("ANTHROPIC_API_BASE")
                    or get_secret("ANTHROPIC_BASE_URL")
                    or "https://api.anthropic.com/v1/messages"
                )
    
                if api_base is not None and not api_base.endswith("/v1/messages"):
                    api_base += "/v1/messages"
    
                response = anthropic_chat_completions.completion(
                    model=model,
                    messages=messages,
                    api_base=api_base,
                    acompletion=acompletion,
                    custom_prompt_dict=litellm.custom_prompt_dict,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,  # for calculating input/output tokens
                    api_key=api_key,
                    logging_obj=logging,
                    headers=headers,
                    timeout=timeout,
                    client=client,
                    custom_llm_provider=custom_llm_provider,
                )
                if optional_params.get("stream", False) or acompletion is True:
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                    )
                response = response
            elif custom_llm_provider == "nlp_cloud":
                nlp_cloud_key = (
                    api_key
                    or litellm.nlp_cloud_key
                    or get_secret("NLP_CLOUD_API_KEY")
                    or litellm.api_key
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("NLP_CLOUD_API_BASE")
                    or "https://api.nlpcloud.io/v1/gpu/"
                )
    
                response = nlp_cloud_chat_completion(
                    model=model,
                    messages=messages,
                    api_base=api_base,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    api_key=nlp_cloud_key,
                    logging_obj=logging,
                )
    
                if "stream" in optional_params and optional_params["stream"] is True:
                    # don't try to access stream object,
                    response = CustomStreamWrapper(
                        response,
                        model,
                        custom_llm_provider="nlp_cloud",
                        logging_obj=logging,
                    )
    
                if optional_params.get("stream", False) or acompletion is True:
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                    )
    
                response = response
            elif custom_llm_provider == "aleph_alpha":
                aleph_alpha_key = (
                    api_key
                    or litellm.aleph_alpha_key
                    or get_secret("ALEPH_ALPHA_API_KEY")
                    or get_secret("ALEPHALPHA_API_KEY")
                    or litellm.api_key
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("ALEPH_ALPHA_API_BASE")
                    or "https://api.aleph-alpha.com/complete"
                )
    
                model_response = aleph_alpha.completion(
                    model=model,
                    messages=messages,
                    api_base=api_base,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    default_max_tokens_to_sample=litellm.max_tokens,
                    api_key=aleph_alpha_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                )
    
                if "stream" in optional_params and optional_params["stream"] is True:
                    # don't try to access stream object,
                    response = CustomStreamWrapper(
                        model_response,
                        model,
                        custom_llm_provider="aleph_alpha",
                        logging_obj=logging,
                    )
                    return response
                response = model_response
            elif custom_llm_provider == "cohere":
                cohere_key = (
                    api_key
                    or litellm.cohere_key
                    or get_secret("COHERE_API_KEY")
                    or get_secret("CO_API_KEY")
                    or litellm.api_key
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("COHERE_API_BASE")
                    or "https://api.cohere.ai/v1/generate"
                )
    
                headers = headers or litellm.headers or {}
                if headers is None:
                    headers = {}
    
                if extra_headers is not None:
                    headers.update(extra_headers)
    
                response = base_llm_http_handler.completion(
                    model=model,
                    stream=stream,
                    messages=messages,
                    acompletion=acompletion,
                    api_base=api_base,
                    model_response=model_response,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_llm_provider="cohere",
                    timeout=timeout,
                    headers=headers,
                    encoding=encoding,
                    api_key=cohere_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                    client=client,
                )
            elif custom_llm_provider == "cohere_chat":
                cohere_key = (
                    api_key
                    or litellm.cohere_key
                    or get_secret_str("COHERE_API_KEY")
                    or get_secret_str("CO_API_KEY")
                    or litellm.api_key
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret_str("COHERE_API_BASE")
                    or "https://api.cohere.ai/v1/chat"
                )
    
                headers = headers or litellm.headers or {}
                if headers is None:
                    headers = {}
    
                if extra_headers is not None:
                    headers.update(extra_headers)
    
                response = base_llm_http_handler.completion(
                    model=model,
                    stream=stream,
                    messages=messages,
                    acompletion=acompletion,
                    api_base=api_base,
                    model_response=model_response,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_llm_provider="cohere_chat",
                    timeout=timeout,
                    headers=headers,
                    encoding=encoding,
                    api_key=cohere_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                )
            elif custom_llm_provider == "maritalk":
                maritalk_key = (
                    api_key
                    or litellm.maritalk_key
                    or get_secret("MARITALK_API_KEY")
                    or litellm.api_key
                )
    
                api_base = (
                    api_base
                    or litellm.api_base
                    or get_secret("MARITALK_API_BASE")
                    or "https://chat.maritaca.ai/api"
                )
    
                model_response = openai_like_chat_completion.completion(
                    model=model,
                    messages=messages,
                    api_base=api_base,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    api_key=maritalk_key,
                    logging_obj=logging,
                    custom_llm_provider="maritalk",
                    custom_prompt_dict=custom_prompt_dict,
                )
    
                response = model_response
            elif custom_llm_provider == "huggingface":
                custom_llm_provider = "huggingface"
                huggingface_key = (
                    api_key
                    or litellm.huggingface_key
                    or os.environ.get("HF_TOKEN")
                    or os.environ.get("HUGGINGFACE_API_KEY")
                    or litellm.api_key
                )
                hf_headers = headers or litellm.headers
    
                custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
                model_response = huggingface.completion(
                    model=model,
                    messages=messages,
                    api_base=api_base,  # type: ignore
                    headers=hf_headers or {},
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    api_key=huggingface_key,
                    acompletion=acompletion,
                    logging_obj=logging,
                    custom_prompt_dict=custom_prompt_dict,
                    timeout=timeout,  # type: ignore
                    client=client,
                )
                if (
                    "stream" in optional_params
                    and optional_params["stream"] is True
                    and acompletion is False
                ):
                    # don't try to access stream object,
                    response = CustomStreamWrapper(
                        model_response,
                        model,
                        custom_llm_provider="huggingface",
                        logging_obj=logging,
                    )
                    return response
                response = model_response
            elif custom_llm_provider == "oobabooga":
                custom_llm_provider = "oobabooga"
                model_response = oobabooga.completion(
                    model=model,
                    messages=messages,
                    model_response=model_response,
                    api_base=api_base,  # type: ignore
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    api_key=None,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    logging_obj=logging,
                )
                if "stream" in optional_params and optional_params["stream"] is True:
                    # don't try to access stream object,
                    response = CustomStreamWrapper(
                        model_response,
                        model,
                        custom_llm_provider="oobabooga",
                        logging_obj=logging,
                    )
                    return response
                response = model_response
            elif custom_llm_provider == "databricks":
                api_base = (
                    api_base  # for databricks we check in get_llm_provider and pass in the api base from there
                    or litellm.api_base
                    or os.getenv("DATABRICKS_API_BASE")
                )
    
                # set API KEY
                api_key = (
                    api_key
                    or litellm.api_key  # for databricks we check in get_llm_provider and pass in the api key from there
                    or litellm.databricks_key
                    or get_secret("DATABRICKS_API_KEY")
                )
    
                headers = headers or litellm.headers
    
                ## COMPLETION CALL
                try:
                    response = databricks_chat_completions.completion(
                        model=model,
                        messages=messages,
                        headers=headers,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        api_key=api_key,
                        api_base=api_base,
                        acompletion=acompletion,
                        logging_obj=logging,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        timeout=timeout,  # type: ignore
                        custom_prompt_dict=custom_prompt_dict,
                        client=client,  # pass AsyncOpenAI, OpenAI client
                        encoding=encoding,
                        custom_llm_provider="databricks",
                    )
                except Exception as e:
                    ## LOGGING - log the original exception returned
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=str(e),
                        additional_args={"headers": headers},
                    )
                    raise e
    
                if optional_params.get("stream", False):
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=api_key,
                        original_response=response,
                        additional_args={"headers": headers},
                    )
            elif custom_llm_provider == "openrouter":
                api_base = api_base or litellm.api_base or "https://openrouter.ai/api/v1"
    
                api_key = (
                    api_key
                    or litellm.api_key
                    or litellm.openrouter_key
                    or get_secret("OPENROUTER_API_KEY")
                    or get_secret("OR_API_KEY")
                )
    
                openrouter_site_url = get_secret("OR_SITE_URL") or "https://litellm.ai"
                openrouter_app_name = get_secret("OR_APP_NAME") or "liteLLM"
    
                openrouter_headers = {
                    "HTTP-Referer": openrouter_site_url,
                    "X-Title": openrouter_app_name,
                }
    
                _headers = headers or litellm.headers
                if _headers:
                    openrouter_headers.update(_headers)
    
                headers = openrouter_headers
    
                ## Load Config
                config = litellm.OpenrouterConfig.get_config()
                for k, v in config.items():
                    if k == "extra_body":
                        # we use openai 'extra_body' to pass openrouter specific params - transforms, route, models
                        if "extra_body" in optional_params:
                            optional_params[k].update(v)
                        else:
                            optional_params[k] = v
                    elif k not in optional_params:
                        optional_params[k] = v
    
                data = {"model": model, "messages": messages, **optional_params}
    
                ## COMPLETION CALL
                response = openai_chat_completions.completion(
                    model=model,
                    messages=messages,
                    headers=headers,
                    api_key=api_key,
                    api_base=api_base,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    logging_obj=logging,
                    acompletion=acompletion,
                    timeout=timeout,  # type: ignore
                    custom_llm_provider="openrouter",
                )
                ## LOGGING
                logging.post_call(
                    input=messages, api_key=openai.api_key, original_response=response
                )
            elif (
                custom_llm_provider == "together_ai"
                or ("togethercomputer" in model)
                or (model in litellm.together_ai_models)
            ):
                """
                Deprecated. We now do together ai calls via the openai client - https://docs.together.ai/docs/openai-api-compatibility
                """
                pass
            elif custom_llm_provider == "palm":
                raise ValueError(
                    "Palm was decommisioned on October 2024. Please use the `gemini/` route for Gemini Google AI Studio Models. Announcement: https://ai.google.dev/palm_docs/palm?hl=en"
                )
            elif custom_llm_provider == "vertex_ai_beta" or custom_llm_provider == "gemini":
                vertex_ai_project = (
                    optional_params.pop("vertex_project", None)
                    or optional_params.pop("vertex_ai_project", None)
                    or litellm.vertex_project
                    or get_secret("VERTEXAI_PROJECT")
                )
                vertex_ai_location = (
                    optional_params.pop("vertex_location", None)
                    or optional_params.pop("vertex_ai_location", None)
                    or litellm.vertex_location
                    or get_secret("VERTEXAI_LOCATION")
                )
                vertex_credentials = (
                    optional_params.pop("vertex_credentials", None)
                    or optional_params.pop("vertex_ai_credentials", None)
                    or get_secret("VERTEXAI_CREDENTIALS")
                )
    
                gemini_api_key = (
                    api_key
                    or get_secret("GEMINI_API_KEY")
                    or get_secret("PALM_API_KEY")  # older palm api key should also work
                    or litellm.api_key
                )
    
                new_params = deepcopy(optional_params)
                response = vertex_chat_completion.completion(  # type: ignore
                    model=model,
                    messages=messages,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=new_params,
                    litellm_params=litellm_params,  # type: ignore
                    logger_fn=logger_fn,
                    encoding=encoding,
                    vertex_location=vertex_ai_location,
                    vertex_project=vertex_ai_project,
                    vertex_credentials=vertex_credentials,
                    gemini_api_key=gemini_api_key,
                    logging_obj=logging,
                    acompletion=acompletion,
                    timeout=timeout,
                    custom_llm_provider=custom_llm_provider,
                    client=client,
                    api_base=api_base,
                    extra_headers=extra_headers,
                )
    
            elif custom_llm_provider == "vertex_ai":
                vertex_ai_project = (
                    optional_params.pop("vertex_project", None)
                    or optional_params.pop("vertex_ai_project", None)
                    or litellm.vertex_project
                    or get_secret("VERTEXAI_PROJECT")
                )
                vertex_ai_location = (
                    optional_params.pop("vertex_location", None)
                    or optional_params.pop("vertex_ai_location", None)
                    or litellm.vertex_location
                    or get_secret("VERTEXAI_LOCATION")
                )
                vertex_credentials = (
                    optional_params.pop("vertex_credentials", None)
                    or optional_params.pop("vertex_ai_credentials", None)
                    or get_secret("VERTEXAI_CREDENTIALS")
                )
    
                new_params = deepcopy(optional_params)
                if (
                    model.startswith("meta/")
                    or model.startswith("mistral")
                    or model.startswith("codestral")
                    or model.startswith("jamba")
                    or model.startswith("claude")
                ):
                    model_response = vertex_partner_models_chat_completion.completion(
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=new_params,
                        litellm_params=litellm_params,  # type: ignore
                        logger_fn=logger_fn,
                        encoding=encoding,
                        api_base=api_base,
                        vertex_location=vertex_ai_location,
                        vertex_project=vertex_ai_project,
                        vertex_credentials=vertex_credentials,
                        logging_obj=logging,
                        acompletion=acompletion,
                        headers=headers,
                        custom_prompt_dict=custom_prompt_dict,
                        timeout=timeout,
                        client=client,
                    )
                elif "gemini" in model or (
                    litellm_params.get("base_model") is not None
                    and "gemini" in litellm_params["base_model"]
                ):
                    model_response = vertex_chat_completion.completion(  # type: ignore
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=new_params,
                        litellm_params=litellm_params,  # type: ignore
                        logger_fn=logger_fn,
                        encoding=encoding,
                        vertex_location=vertex_ai_location,
                        vertex_project=vertex_ai_project,
                        vertex_credentials=vertex_credentials,
                        gemini_api_key=None,
                        logging_obj=logging,
                        acompletion=acompletion,
                        timeout=timeout,
                        custom_llm_provider=custom_llm_provider,
                        client=client,
                        api_base=api_base,
                        extra_headers=extra_headers,
                    )
                elif "openai" in model:
                    # Vertex Model Garden - OpenAI compatible models
                    model_response = vertex_model_garden_chat_completion.completion(
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=new_params,
                        litellm_params=litellm_params,  # type: ignore
                        logger_fn=logger_fn,
                        encoding=encoding,
                        api_base=api_base,
                        vertex_location=vertex_ai_location,
                        vertex_project=vertex_ai_project,
                        vertex_credentials=vertex_credentials,
                        logging_obj=logging,
                        acompletion=acompletion,
                        headers=headers,
                        custom_prompt_dict=custom_prompt_dict,
                        timeout=timeout,
                        client=client,
                    )
                else:
                    model_response = vertex_ai_non_gemini.completion(
                        model=model,
                        messages=messages,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=new_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        encoding=encoding,
                        vertex_location=vertex_ai_location,
                        vertex_project=vertex_ai_project,
                        vertex_credentials=vertex_credentials,
                        logging_obj=logging,
                        acompletion=acompletion,
                    )
    
                    if (
                        "stream" in optional_params
                        and optional_params["stream"] is True
                        and acompletion is False
                    ):
                        response = CustomStreamWrapper(
                            model_response,
                            model,
                            custom_llm_provider="vertex_ai",
                            logging_obj=logging,
                        )
                        return response
                response = model_response
            elif custom_llm_provider == "predibase":
                tenant_id = (
                    optional_params.pop("tenant_id", None)
                    or optional_params.pop("predibase_tenant_id", None)
                    or litellm.predibase_tenant_id
                    or get_secret("PREDIBASE_TENANT_ID")
                )
    
                if tenant_id is None:
                    raise ValueError(
                        "Missing Predibase Tenant ID - Required for making the request. Set dynamically (e.g. `completion(..tenant_id=<MY-ID>)`) or in env - `PREDIBASE_TENANT_ID`."
                    )
    
                api_base = (
                    api_base
                    or optional_params.pop("api_base", None)
                    or optional_params.pop("base_url", None)
                    or litellm.api_base
                    or get_secret("PREDIBASE_API_BASE")
                )
    
                api_key = (
                    api_key
                    or litellm.api_key
                    or litellm.predibase_key
                    or get_secret("PREDIBASE_API_KEY")
                )
    
                _model_response = predibase_chat_completions.completion(
                    model=model,
                    messages=messages,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    logging_obj=logging,
                    acompletion=acompletion,
                    api_base=api_base,
                    custom_prompt_dict=custom_prompt_dict,
                    api_key=api_key,
                    tenant_id=tenant_id,
                    timeout=timeout,
                )
    
                if (
                    "stream" in optional_params
                    and optional_params["stream"] is True
                    and acompletion is False
                ):
                    return _model_response
                response = _model_response
            elif custom_llm_provider == "text-completion-codestral":
                api_base = (
                    api_base
                    or optional_params.pop("api_base", None)
                    or optional_params.pop("base_url", None)
                    or litellm.api_base
                    or "https://codestral.mistral.ai/v1/fim/completions"
                )
    
                api_key = api_key or litellm.api_key or get_secret("CODESTRAL_API_KEY")
    
                text_completion_model_response = litellm.TextCompletionResponse(
                    stream=stream
                )
    
                _model_response = codestral_text_completions.completion(  # type: ignore
                    model=model,
                    messages=messages,
                    model_response=text_completion_model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    logging_obj=logging,
                    acompletion=acompletion,
                    api_base=api_base,
                    custom_prompt_dict=custom_prompt_dict,
                    api_key=api_key,
                    timeout=timeout,
                )
    
                if (
                    "stream" in optional_params
                    and optional_params["stream"] is True
                    and acompletion is False
                ):
                    return _model_response
                response = _model_response
            elif custom_llm_provider == "sagemaker_chat":
                # boto3 reads keys from .env
                model_response = sagemaker_chat_completion.completion(
                    model=model,
                    messages=messages,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_prompt_dict=custom_prompt_dict,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    logging_obj=logging,
                    acompletion=acompletion,
                )
    
                ## RESPONSE OBJECT
                response = model_response
            elif custom_llm_provider == "sagemaker":
                # boto3 reads keys from .env
                model_response = sagemaker_llm.completion(
                    model=model,
                    messages=messages,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_prompt_dict=custom_prompt_dict,
                    hf_model_name=hf_model_name,
                    logger_fn=logger_fn,
                    encoding=encoding,
                    logging_obj=logging,
                    acompletion=acompletion,
                )
    
                ## RESPONSE OBJECT
                response = model_response
            elif custom_llm_provider == "bedrock":
                # boto3 reads keys from .env
                custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
    
                if "aws_bedrock_client" in optional_params:
                    verbose_logger.warning(
                        "'aws_bedrock_client' is a deprecated param. Please move to another auth method - https://docs.litellm.ai/docs/providers/bedrock#boto3---authentication."
                    )
                    # Extract credentials for legacy boto3 client and pass thru to httpx
                    aws_bedrock_client = optional_params.pop("aws_bedrock_client")
                    creds = aws_bedrock_client._get_credentials().get_frozen_credentials()
    
                    if creds.access_key:
                        optional_params["aws_access_key_id"] = creds.access_key
                    if creds.secret_key:
                        optional_params["aws_secret_access_key"] = creds.secret_key
                    if creds.token:
                        optional_params["aws_session_token"] = creds.token
                    if (
                        "aws_region_name" not in optional_params
                        or optional_params["aws_region_name"] is None
                    ):
                        optional_params["aws_region_name"] = (
                            aws_bedrock_client.meta.region_name
                        )
    
                base_model = litellm.AmazonConverseConfig()._get_base_model(model)
    
                if base_model in litellm.bedrock_converse_models or model.startswith(
                    "converse/"
                ):
                    model = model.replace("converse/", "")
                    response = bedrock_converse_chat_completion.completion(
                        model=model,
                        messages=messages,
                        custom_prompt_dict=custom_prompt_dict,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=optional_params,
                        litellm_params=litellm_params,  # type: ignore
                        logger_fn=logger_fn,
                        encoding=encoding,
                        logging_obj=logging,
                        extra_headers=extra_headers,
                        timeout=timeout,
                        acompletion=acompletion,
                        client=client,
                        api_base=api_base,
                    )
                else:
                    model = model.replace("invoke/", "")
                    response = bedrock_chat_completion.completion(
                        model=model,
                        messages=messages,
                        custom_prompt_dict=custom_prompt_dict,
                        model_response=model_response,
                        print_verbose=print_verbose,
                        optional_params=optional_params,
                        litellm_params=litellm_params,
                        logger_fn=logger_fn,
                        encoding=encoding,
                        logging_obj=logging,
                        extra_headers=extra_headers,
                        timeout=timeout,
                        acompletion=acompletion,
                        client=client,
                        api_base=api_base,
                    )
    
                if optional_params.get("stream", False):
                    ## LOGGING
                    logging.post_call(
                        input=messages,
                        api_key=None,
                        original_response=response,
                    )
    
                ## RESPONSE OBJECT
                response = response
            elif custom_llm_provider == "watsonx":
                response = watsonx_chat_completion.completion(
                    model=model,
                    messages=messages,
                    headers=headers,
                    model_response=model_response,
                    print_verbose=print_verbose,
                    api_key=api_key,
                    api_base=api_base,
                    acompletion=acompletion,
                    logging_obj=logging,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    logger_fn=logger_fn,
                    timeout=timeout,  # type: ignore
                    custom_prompt_dict=custom_prompt_dict,
                    client=client,  # pass AsyncOpenAI, OpenAI client
                    encoding=encoding,
                    custom_llm_provider="watsonx",
                )
            elif custom_llm_provider == "watsonx_text":
                api_key = (
                    api_key
                    or optional_params.pop("apikey", None)
                    or get_secret_str("WATSONX_APIKEY")
                    or get_secret_str("WATSONX_API_KEY")
                    or get_secret_str("WX_API_KEY")
                )
    
                api_base = (
                    api_base
                    or optional_params.pop(
                        "url",
                        optional_params.pop(
                            "api_base", optional_params.pop("base_url", None)
                        ),
                    )
                    or get_secret_str("WATSONX_API_BASE")
                    or get_secret_str("WATSONX_URL")
                    or get_secret_str("WX_URL")
                    or get_secret_str("WML_URL")
                )
    
                wx_credentials = optional_params.pop(
                    "wx_credentials",
                    optional_params.pop(
                        "watsonx_credentials", None
                    ),  # follow {provider}_credentials, same as vertex ai
                )
    
                token: Optional[str] = None
                if wx_credentials is not None:
                    api_base = wx_credentials.get("url", api_base)
                    api_key = wx_credentials.get(
                        "apikey", wx_credentials.get("api_key", api_key)
                    )
                    token = wx_credentials.get(
                        "token",
                        wx_credentials.get(
                            "watsonx_token", None
                        ),  # follow format of {provider}_token, same as azure - e.g. 'azure_ad_token=..'
                    )
    
                if token is not None:
                    optional_params["token"] = token
    
>               response = base_llm_http_handler.completion(
                    model=model,
                    stream=stream,
                    messages=messages,
                    acompletion=acompletion,
                    api_base=api_base,
                    model_response=model_response,
                    optional_params=optional_params,
                    litellm_params=litellm_params,
                    custom_llm_provider="watsonx_text",
                    timeout=timeout,
                    headers=headers,
                    encoding=encoding,
                    api_key=api_key,
                    logging_obj=logging,  # model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements
                    client=client,

../../litellm/main.py:2633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../litellm/llms/custom_httpx/llm_http_handler.py:225: in completion
    return provider_config.transform_response(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <litellm.llms.watsonx.completion.transformation.IBMWatsonXAIConfig object at 0x1241412d0>
model = 'ibm/granite-13b-chat-v2', raw_response = <Response [200 OK]>
model_response = ModelResponse(id='chatcmpl-b196d32c-c63f-4ecc-9734-aeebc6218398', created=1734807216, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))
logging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x124477750>
request_data = {'input': '<|user|>\nWrite a short poem about the sky\n<|assistant|>', 'model_id': 'ibm/granite-13b-chat-v2', 'moderations': {}, 'parameters': {'max_new_tokens': 20, 'stop_sequences': ['stop']}, ...}
messages = [{'content': 'Write a short poem about the sky', 'role': 'user'}]
optional_params = {'max_new_tokens': 20, 'stop_sequences': ['stop']}
litellm_params = {'acompletion': False, 'api_base': 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-03-13', 'api_key': None, 'azure_ad_token_provider': None, ...}
encoding = <Encoding 'cl100k_base'>
api_key = 'hGBBF6nx-tJRzXcN_NereGOPb_J8lyguQc5cceQeRBec', json_mode = None

    def transform_response(
        self,
        model: str,
        raw_response: httpx.Response,
        model_response: ModelResponse,
        logging_obj: LiteLLMLoggingObj,
        request_data: Dict,
        messages: List[AllMessageValues],
        optional_params: Dict,
        litellm_params: Dict,
        encoding: str,
        api_key: Optional[str] = None,
        json_mode: Optional[bool] = None,
    ) -> ModelResponse:
>       raise NotImplementedError(
            "transform_response not implemented. Done in watsonx/completion handler.py"
        )
E       NotImplementedError: transform_response not implemented. Done in watsonx/completion handler.py

../../litellm/llms/watsonx/completion/transformation.py:271: NotImplementedError

During handling of the above exception, another exception occurred:

    def test_completion_watsonx_error():
        litellm.set_verbose = True
        model_name = "watsonx_text/ibm/granite-13b-chat-v2"
    
>       response = completion(
            model=model_name,
            messages=messages,
            stop=["stop"],
            max_tokens=20,
        )

test_completion.py:4024: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../litellm/utils.py:1005: in wrapper
    raise e
../../litellm/utils.py:886: in wrapper
    result = original_function(*args, **kwargs)
../../litellm/main.py:2963: in completion
    raise exception_type(
../../litellm/litellm_core_utils/exception_mapping_utils.py:2146: in exception_type
    raise e
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = 'ibm/granite-13b-chat-v2'
original_exception = NotImplementedError('transform_response not implemented. Done in watsonx/completion handler.py')
custom_llm_provider = 'watsonx_text'
completion_kwargs = {'acompletion': False, 'api_base': 'https://us-south.ml.cloud.ibm.com', 'api_key': 'hGBBF6nx-tJRzXcN_NereGOPb_J8lyguQc5cceQeRBec', 'api_version': None, ...}
extra_kwargs = {'litellm_call_id': 'df6d0a8e-daf4-49a5-9b55-5237c36d0251', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x124477750>}

    def exception_type(  # type: ignore  # noqa: PLR0915
        model,
        original_exception,
        custom_llm_provider,
        completion_kwargs={},
        extra_kwargs={},
    ):
    
        if any(
            isinstance(original_exception, exc_type)
            for exc_type in litellm.LITELLM_EXCEPTION_TYPES
        ):
            return original_exception
        exception_mapping_worked = False
        exception_provider = custom_llm_provider
        if litellm.suppress_debug_info is False:
            print()  # noqa
            print(  # noqa
                "\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\033[0m"  # noqa
            )  # noqa
            print(  # noqa
                "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'."  # noqa
            )  # noqa
            print()  # noqa
    
        litellm_response_headers = _get_response_headers(
            original_exception=original_exception
        )
        try:
            if model:
                if hasattr(original_exception, "message"):
                    error_str = str(original_exception.message)
                else:
                    error_str = str(original_exception)
                if isinstance(original_exception, BaseException):
                    exception_type = type(original_exception).__name__
                else:
                    exception_type = ""
    
                ################################################################################
                # Common Extra information needed for all providers
                # We pass num retries, api_base, vertex_deployment etc to the exception here
                ################################################################################
                extra_information = ""
                try:
                    _api_base = litellm.get_api_base(
                        model=model, optional_params=extra_kwargs
                    )
                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)
                    _vertex_project = extra_kwargs.get("vertex_project")
                    _vertex_location = extra_kwargs.get("vertex_location")
                    _metadata = extra_kwargs.get("metadata", {}) or {}
                    _model_group = _metadata.get("model_group")
                    _deployment = _metadata.get("deployment")
                    extra_information = f"\nModel: {model}"
    
                    if (
                        isinstance(custom_llm_provider, str)
                        and len(custom_llm_provider) > 0
                    ):
                        exception_provider = (
                            custom_llm_provider[0].upper()
                            + custom_llm_provider[1:]
                            + "Exception"
                        )
    
                    if _api_base:
                        extra_information += f"\nAPI Base: `{_api_base}`"
                    if (
                        messages
                        and len(messages) > 0
                        and litellm.redact_messages_in_exceptions is False
                    ):
                        extra_information += f"\nMessages: `{messages}`"
    
                    if _model_group is not None:
                        extra_information += f"\nmodel_group: `{_model_group}`\n"
                    if _deployment is not None:
                        extra_information += f"\ndeployment: `{_deployment}`\n"
                    if _vertex_project is not None:
                        extra_information += f"\nvertex_project: `{_vertex_project}`\n"
                    if _vertex_location is not None:
                        extra_information += f"\nvertex_location: `{_vertex_location}`\n"
    
                    # on litellm proxy add key name + team to exceptions
                    extra_information = _add_key_name_and_team_to_alert(
                        request_info=extra_information, metadata=_metadata
                    )
                except Exception:
                    # DO NOT LET this Block raising the original exception
                    pass
    
                ################################################################################
                # End of Common Extra information Needed for all providers
                ################################################################################
    
                ################################################################################
                #################### Start of Provider Exception mapping ####################
                ################################################################################
    
                if (
                    "Request Timeout Error" in error_str
                    or "Request timed out" in error_str
                    or "Timed out generating response" in error_str
                ):
                    exception_mapping_worked = True
                    raise Timeout(
                        message=f"APITimeoutError - Request timed out. \nerror_str: {error_str}",
                        model=model,
                        llm_provider=custom_llm_provider,
                        litellm_debug_info=extra_information,
                    )
    
                if (
                    custom_llm_provider == "openai"
                    or custom_llm_provider == "text-completion-openai"
                    or custom_llm_provider == "custom_openai"
                    or custom_llm_provider in litellm.openai_compatible_providers
                ):
                    # custom_llm_provider is openai, make it OpenAI
                    message = get_error_message(error_obj=original_exception)
                    if message is None:
                        if hasattr(original_exception, "message"):
                            message = original_exception.message
                        else:
                            message = str(original_exception)
    
                    if message is not None and isinstance(
                        message, str
                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414
                        message = message.replace("OPENAI", custom_llm_provider.upper())
                        message = message.replace(
                            "openai.OpenAIError",
                            "{}.{}Error".format(custom_llm_provider, custom_llm_provider),
                        )
                    if custom_llm_provider == "openai":
                        exception_provider = "OpenAI" + "Exception"
                    else:
                        exception_provider = (
                            custom_llm_provider[0].upper()
                            + custom_llm_provider[1:]
                            + "Exception"
                        )
    
                    if (
                        "This model's maximum context length is" in error_str
                        or "string too long. Expected a string with maximum length"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"ContextWindowExceededError: {exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "invalid_request_error" in error_str
                        and "model_not_found" in error_str
                    ):
                        exception_mapping_worked = True
                        raise NotFoundError(
                            message=f"{exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif "A timeout occurred" in error_str:
                        exception_mapping_worked = True
                        raise Timeout(
                            message=f"{exception_provider} - {message}",
                            model=model,
                            llm_provider=custom_llm_provider,
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "invalid_request_error" in error_str
                        and "content_policy_violation" in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContentPolicyViolationError(
                            message=f"ContentPolicyViolationError: {exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "invalid_request_error" in error_str
                        and "Incorrect API key provided" not in error_str
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"{exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "Web server is returning an unknown error" in error_str
                        or "The server had an error processing your request." in error_str
                    ):
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"{exception_provider} - {message}",
                            model=model,
                            llm_provider=custom_llm_provider,
                        )
                    elif "Request too large" in error_str:
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"RateLimitError: {exception_provider} - {message}",
                            model=model,
                            llm_provider=custom_llm_provider,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"AuthenticationError: {exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif "Mistral API raised a streaming error" in error_str:
                        exception_mapping_worked = True
                        _request = httpx.Request(
                            method="POST", url="https://api.openai.com/v1"
                        )
                        raise APIError(
                            status_code=500,
                            message=f"{exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            request=_request,
                            litellm_debug_info=extra_information,
                        )
                    elif hasattr(original_exception, "status_code"):
                        exception_mapping_worked = True
                        if original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"{exception_provider} - {message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AuthenticationError: {exception_provider} - {message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"NotFoundError: {exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"Timeout Error: {exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"{exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"RateLimitError: {exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"ServiceUnavailableError: {exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"Timeout Error: {exception_provider} - {message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"APIError: {exception_provider} - {message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                request=getattr(original_exception, "request", None),
                                litellm_debug_info=extra_information,
                            )
                    else:
                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors
                        # exception_mapping_worked = True
                        raise APIConnectionError(
                            message=f"APIConnectionError: {exception_provider} - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            litellm_debug_info=extra_information,
                            request=httpx.Request(
                                method="POST", url="https://api.openai.com/v1/"
                            ),
                        )
                elif custom_llm_provider == "anthropic":  # one of the anthropics
                    if "prompt is too long" in error_str or "prompt: length" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message="AnthropicError - {}".format(error_str),
                            model=model,
                            llm_provider="anthropic",
                        )
                    if "Invalid API Key" in error_str:
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message="AnthropicError - {}".format(error_str),
                            model=model,
                            llm_provider="anthropic",
                        )
                    if "content filtering policy" in error_str:
                        exception_mapping_worked = True
                        raise ContentPolicyViolationError(
                            message="AnthropicError - {}".format(error_str),
                            model=model,
                            llm_provider="anthropic",
                        )
                    if "Client error '400 Bad Request'" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message="AnthropicError - {}".format(error_str),
                            model=model,
                            llm_provider="anthropic",
                        )
                    if hasattr(original_exception, "status_code"):
                        verbose_logger.debug(
                            f"status_code: {original_exception.status_code}"
                        )
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AnthropicException - {error_str}",
                                llm_provider="anthropic",
                                model=model,
                            )
                        elif (
                            original_exception.status_code == 400
                            or original_exception.status_code == 413
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AnthropicException - {error_str}",
                                model=model,
                                llm_provider="anthropic",
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"AnthropicException - {error_str}",
                                model=model,
                                llm_provider="anthropic",
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"AnthropicException - {error_str}",
                                model=model,
                                llm_provider="anthropic",
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"AnthropicException - {error_str}",
                                llm_provider="anthropic",
                                model=model,
                            )
                        elif (
                            original_exception.status_code == 500
                            or original_exception.status_code == 529
                        ):
                            exception_mapping_worked = True
                            raise litellm.InternalServerError(
                                message=f"AnthropicException - {error_str}. Handle with `litellm.InternalServerError`.",
                                llm_provider="anthropic",
                                model=model,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise litellm.ServiceUnavailableError(
                                message=f"AnthropicException - {error_str}. Handle with `litellm.ServiceUnavailableError`.",
                                llm_provider="anthropic",
                                model=model,
                            )
                elif custom_llm_provider == "replicate":
                    if "Incorrect authentication token" in error_str:
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"ReplicateException - {error_str}",
                            llm_provider="replicate",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "input is too long" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"ReplicateException - {error_str}",
                            model=model,
                            llm_provider="replicate",
                            response=getattr(original_exception, "response", None),
                        )
                    elif exception_type == "ModelError":
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"ReplicateException - {error_str}",
                            model=model,
                            llm_provider="replicate",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Request was throttled" in error_str:
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"ReplicateException - {error_str}",
                            llm_provider="replicate",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"ReplicateException - {original_exception.message}",
                                llm_provider="replicate",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif (
                            original_exception.status_code == 400
                            or original_exception.status_code == 413
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"ReplicateException - {original_exception.message}",
                                model=model,
                                llm_provider="replicate",
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise UnprocessableEntityError(
                                message=f"ReplicateException - {original_exception.message}",
                                model=model,
                                llm_provider="replicate",
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"ReplicateException - {original_exception.message}",
                                model=model,
                                llm_provider="replicate",
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise UnprocessableEntityError(
                                message=f"ReplicateException - {original_exception.message}",
                                llm_provider="replicate",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"ReplicateException - {original_exception.message}",
                                llm_provider="replicate",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"ReplicateException - {original_exception.message}",
                                llm_provider="replicate",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                    exception_mapping_worked = True
                    raise APIError(
                        status_code=500,
                        message=f"ReplicateException - {str(original_exception)}",
                        llm_provider="replicate",
                        model=model,
                        request=httpx.Request(
                            method="POST",
                            url="https://api.replicate.com/v1/deployments",
                        ),
                    )
                elif custom_llm_provider in litellm._openai_like_providers:
                    if "authorization denied for" in error_str:
                        exception_mapping_worked = True
    
                        # Predibase returns the raw API Key in the response - this block ensures it's not returned in the exception
                        if (
                            error_str is not None
                            and isinstance(error_str, str)
                            and "bearer" in error_str.lower()
                        ):
                            # only keep the first 10 chars after the occurnence of "bearer"
                            _bearer_token_start_index = error_str.lower().find("bearer")
                            error_str = error_str[: _bearer_token_start_index + 14]
                            error_str += "XXXXXXX" + '"'
    
                        raise AuthenticationError(
                            message=f"{custom_llm_provider}Exception: Authentication Error - {error_str}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                            litellm_debug_info=extra_information,
                        )
                    elif "token_quota_reached" in error_str:
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"{custom_llm_provider}Exception: Rate Limit Errror - {error_str}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "The server received an invalid response from an upstream server."
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"{custom_llm_provider}Exception - {original_exception.message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                        )
                    elif "model_no_support_for_function" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"{custom_llm_provider}Exception - Use 'watsonx_text' route instead. IBM WatsonX does not support `/text/chat` endpoint. - {error_str}",
                            llm_provider=custom_llm_provider,
                            model=model,
                        )
                    elif hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise litellm.InternalServerError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        elif (
                            original_exception.status_code == 401
                            or original_exception.status_code == 403
                        ):
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        elif original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif (
                            original_exception.status_code == 422
                            or original_exception.status_code == 424
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"{custom_llm_provider}Exception - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                elif custom_llm_provider == "bedrock":
                    if (
                        "too many tokens" in error_str
                        or "expected maxLength:" in error_str
                        or "Input is too long" in error_str
                        or "prompt is too long" in error_str
                        or "prompt: length: 1.." in error_str
                        or "Too many input tokens" in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"BedrockException: Context Window Error - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                        )
                    elif (
                        "Conversation blocks and tool result blocks cannot be provided in the same turn."
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"BedrockException - {error_str}\n. Enable 'litellm.modify_params=True' (for PROXY do: `litellm_settings::modify_params: True`) to insert a dummy assistant message and fix this error.",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Malformed input request" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"BedrockException - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "A conversation must start with a user message." in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"BedrockException - {error_str}\n. Pass in default user message via `completion(..,user_continue_message=)` or enable `litellm.modify_params=True`.\nFor Proxy: do via `litellm_settings::modify_params: True` or user_continue_message under `litellm_params`",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "Unable to locate credentials" in error_str
                        or "The security token included in the request is invalid"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"BedrockException Invalid Authentication - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "AccessDeniedException" in error_str:
                        exception_mapping_worked = True
                        raise PermissionDeniedError(
                            message=f"BedrockException PermissionDeniedError - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "throttlingException" in error_str
                        or "ThrottlingException" in error_str
                    ):
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"BedrockException: Rate Limit Error - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "Connect timeout on endpoint URL" in error_str
                        or "timed out" in error_str
                    ):
                        exception_mapping_worked = True
                        raise Timeout(
                            message=f"BedrockException: Timeout Error - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                        )
                    elif "Could not process image" in error_str:
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"BedrockException - {error_str}",
                            model=model,
                            llm_provider="bedrock",
                        )
                    elif hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"BedrockException - {original_exception.message}",
                                llm_provider="bedrock",
                                model=model,
                                response=httpx.Response(
                                    status_code=500,
                                    request=httpx.Request(
                                        method="POST", url="https://api.openai.com/v1/"
                                    ),
                                ),
                            )
                        elif original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"BedrockException - {original_exception.message}",
                                llm_provider="bedrock",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"BedrockException - {original_exception.message}",
                                llm_provider="bedrock",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"BedrockException - {original_exception.message}",
                                llm_provider="bedrock",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"BedrockException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"BedrockException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"BedrockException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"BedrockException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"BedrockException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                elif (
                    custom_llm_provider == "sagemaker"
                    or custom_llm_provider == "sagemaker_chat"
                ):
                    if "Unable to locate credentials" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"litellm.BadRequestError: SagemakerException - {error_str}",
                            model=model,
                            llm_provider="sagemaker",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "Input validation error: `best_of` must be > 0 and <= 2"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message="SagemakerException - the value of 'n' must be > 0 and <= 2 for sagemaker endpoints",
                            model=model,
                            llm_provider="sagemaker",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "`inputs` tokens + `max_new_tokens` must be <=" in error_str
                        or "instance type with more CPU capacity or memory" in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"SagemakerException - {error_str}",
                            model=model,
                            llm_provider="sagemaker",
                            response=getattr(original_exception, "response", None),
                        )
                    elif hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"SagemakerException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=httpx.Response(
                                    status_code=500,
                                    request=httpx.Request(
                                        method="POST", url="https://api.openai.com/v1/"
                                    ),
                                ),
                            )
                        elif original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"SagemakerException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"SagemakerException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"SagemakerException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"SagemakerException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif (
                            original_exception.status_code == 422
                            or original_exception.status_code == 424
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"SagemakerException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"SagemakerException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"SagemakerException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"SagemakerException - {original_exception.message}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                elif (
                    custom_llm_provider == "vertex_ai"
                    or custom_llm_provider == "vertex_ai_beta"
                    or custom_llm_provider == "gemini"
                ):
                    if (
                        "Vertex AI API has not been used in project" in error_str
                        or "Unable to find your project" in error_str
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"litellm.BadRequestError: VertexAIException - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            response=httpx.Response(
                                status_code=400,
                                request=httpx.Request(
                                    method="POST",
                                    url=" https://cloud.google.com/vertex-ai/",
                                ),
                            ),
                            litellm_debug_info=extra_information,
                        )
                    if "400 Request payload size exceeds" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"VertexException - {error_str}",
                            model=model,
                            llm_provider=custom_llm_provider,
                        )
                    elif (
                        "None Unknown Error." in error_str
                        or "Content has no parts." in error_str
                    ):
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"litellm.InternalServerError: VertexAIException - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            response=httpx.Response(
                                status_code=500,
                                content=str(original_exception),
                                request=httpx.Request(method="completion", url="https://github.com/BerriAI/litellm"),  # type: ignore
                            ),
                            litellm_debug_info=extra_information,
                        )
                    elif "API key not valid." in error_str:
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"{custom_llm_provider}Exception - {error_str}",
                            model=model,
                            llm_provider=custom_llm_provider,
                            litellm_debug_info=extra_information,
                        )
                    elif "403" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"VertexAIException BadRequestError - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            response=httpx.Response(
                                status_code=403,
                                request=httpx.Request(
                                    method="POST",
                                    url=" https://cloud.google.com/vertex-ai/",
                                ),
                            ),
                            litellm_debug_info=extra_information,
                        )
                    elif (
                        "The response was blocked." in error_str
                        or "Output blocked by content filtering policy"
                        in error_str  # anthropic on vertex ai
                    ):
                        exception_mapping_worked = True
                        raise ContentPolicyViolationError(
                            message=f"VertexAIException ContentPolicyViolationError - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            litellm_debug_info=extra_information,
                            response=httpx.Response(
                                status_code=400,
                                request=httpx.Request(
                                    method="POST",
                                    url=" https://cloud.google.com/vertex-ai/",
                                ),
                            ),
                        )
                    elif (
                        "429 Quota exceeded" in error_str
                        or "Quota exceeded for" in error_str
                        or "IndexError: list index out of range" in error_str
                        or "429 Unable to submit request because the service is temporarily out of capacity."
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"litellm.RateLimitError: VertexAIException - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            litellm_debug_info=extra_information,
                            response=httpx.Response(
                                status_code=429,
                                request=httpx.Request(
                                    method="POST",
                                    url=" https://cloud.google.com/vertex-ai/",
                                ),
                            ),
                        )
                    elif (
                        "500 Internal Server Error" in error_str
                        or "The model is overloaded." in error_str
                    ):
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"litellm.InternalServerError: VertexAIException - {error_str}",
                            model=model,
                            llm_provider="vertex_ai",
                            litellm_debug_info=extra_information,
                        )
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"VertexAIException BadRequestError - {error_str}",
                                model=model,
                                llm_provider="vertex_ai",
                                litellm_debug_info=extra_information,
                                response=httpx.Response(
                                    status_code=400,
                                    request=httpx.Request(
                                        method="POST",
                                        url="https://cloud.google.com/vertex-ai/",
                                    ),
                                ),
                            )
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"VertexAIException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        if original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"VertexAIException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                        if original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"VertexAIException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
    
                        if original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"litellm.RateLimitError: VertexAIException - {error_str}",
                                model=model,
                                llm_provider="vertex_ai",
                                litellm_debug_info=extra_information,
                                response=httpx.Response(
                                    status_code=429,
                                    request=httpx.Request(
                                        method="POST",
                                        url=" https://cloud.google.com/vertex-ai/",
                                    ),
                                ),
                            )
                        if original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise litellm.InternalServerError(
                                message=f"VertexAIException InternalServerError - {error_str}",
                                model=model,
                                llm_provider="vertex_ai",
                                litellm_debug_info=extra_information,
                                response=httpx.Response(
                                    status_code=500,
                                    content=str(original_exception),
                                    request=httpx.Request(method="completion", url="https://github.com/BerriAI/litellm"),  # type: ignore
                                ),
                            )
                        if original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"VertexAIException - {original_exception.message}",
                                llm_provider=custom_llm_provider,
                                model=model,
                            )
                elif custom_llm_provider == "palm" or custom_llm_provider == "gemini":
                    if "503 Getting metadata" in error_str:
                        # auth errors look like this
                        # 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message="GeminiException - Invalid api key",
                            model=model,
                            llm_provider="palm",
                            response=getattr(original_exception, "response", None),
                        )
                    if (
                        "504 Deadline expired before operation could complete." in error_str
                        or "504 Deadline Exceeded" in error_str
                    ):
                        exception_mapping_worked = True
                        raise Timeout(
                            message=f"GeminiException - {original_exception.message}",
                            model=model,
                            llm_provider="palm",
                        )
                    if "400 Request payload size exceeds" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"GeminiException - {error_str}",
                            model=model,
                            llm_provider="palm",
                            response=getattr(original_exception, "response", None),
                        )
                    if (
                        "500 An internal error has occurred." in error_str
                        or "list index out of range" in error_str
                    ):
                        exception_mapping_worked = True
                        raise APIError(
                            status_code=getattr(original_exception, "status_code", 500),
                            message=f"GeminiException - {original_exception.message}",
                            llm_provider="palm",
                            model=model,
                            request=httpx.Response(
                                status_code=429,
                                request=httpx.Request(
                                    method="POST",
                                    url=" https://cloud.google.com/vertex-ai/",
                                ),
                            ),
                        )
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"GeminiException - {error_str}",
                                model=model,
                                llm_provider="palm",
                                response=getattr(original_exception, "response", None),
                            )
                    # Dailed: Error occurred: 400 Request payload size exceeds the limit: 20000 bytes
                elif custom_llm_provider == "cloudflare":
                    if "Authentication error" in error_str:
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"Cloudflare Exception - {original_exception.message}",
                            llm_provider="cloudflare",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    if "must have required property" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"Cloudflare Exception - {original_exception.message}",
                            llm_provider="cloudflare",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                elif (
                    custom_llm_provider == "cohere" or custom_llm_provider == "cohere_chat"
                ):  # Cohere
                    if (
                        "invalid api token" in error_str
                        or "No API key provided." in error_str
                    ):
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"CohereException - {original_exception.message}",
                            llm_provider="cohere",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "too many tokens" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"CohereException - {original_exception.message}",
                            model=model,
                            llm_provider="cohere",
                            response=getattr(original_exception, "response", None),
                        )
                    elif hasattr(original_exception, "status_code"):
                        if (
                            original_exception.status_code == 400
                            or original_exception.status_code == 498
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"CohereException - {original_exception.message}",
                                llm_provider="cohere",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"CohereException - {original_exception.message}",
                                llm_provider="cohere",
                                model=model,
                            )
                        elif original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"CohereException - {original_exception.message}",
                                llm_provider="cohere",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                    elif (
                        "CohereConnectionError" in exception_type
                    ):  # cohere seems to fire these errors when we load test it (1k+ messages / min)
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=f"CohereException - {original_exception.message}",
                            llm_provider="cohere",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "invalid type:" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"CohereException - {original_exception.message}",
                            llm_provider="cohere",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Unexpected server error" in error_str:
                        exception_mapping_worked = True
                        raise ServiceUnavailableError(
                            message=f"CohereException - {original_exception.message}",
                            llm_provider="cohere",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    else:
                        if hasattr(original_exception, "status_code"):
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"CohereException - {original_exception.message}",
                                llm_provider="cohere",
                                model=model,
                                request=original_exception.request,
                            )
                        raise original_exception
                elif custom_llm_provider == "huggingface":
                    if "length limit exceeded" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=error_str,
                            model=model,
                            llm_provider="huggingface",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "A valid user token is required" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=error_str,
                            llm_provider="huggingface",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Rate limit reached" in error_str:
                        exception_mapping_worked = True
                        raise RateLimitError(
                            message=error_str,
                            llm_provider="huggingface",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"HuggingfaceException - {original_exception.message}",
                                llm_provider="huggingface",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"HuggingfaceException - {original_exception.message}",
                                model=model,
                                llm_provider="huggingface",
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"HuggingfaceException - {original_exception.message}",
                                model=model,
                                llm_provider="huggingface",
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"HuggingfaceException - {original_exception.message}",
                                llm_provider="huggingface",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"HuggingfaceException - {original_exception.message}",
                                llm_provider="huggingface",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"HuggingfaceException - {original_exception.message}",
                                llm_provider="huggingface",
                                model=model,
                                request=original_exception.request,
                            )
                elif custom_llm_provider == "ai21":
                    if hasattr(original_exception, "message"):
                        if "Prompt has too many tokens" in original_exception.message:
                            exception_mapping_worked = True
                            raise ContextWindowExceededError(
                                message=f"AI21Exception - {original_exception.message}",
                                model=model,
                                llm_provider="ai21",
                                response=getattr(original_exception, "response", None),
                            )
                        if "Bad or missing API token." in original_exception.message:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AI21Exception - {original_exception.message}",
                                model=model,
                                llm_provider="ai21",
                                response=getattr(original_exception, "response", None),
                            )
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AI21Exception - {original_exception.message}",
                                llm_provider="ai21",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"AI21Exception - {original_exception.message}",
                                model=model,
                                llm_provider="ai21",
                            )
                        if original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AI21Exception - {original_exception.message}",
                                model=model,
                                llm_provider="ai21",
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"AI21Exception - {original_exception.message}",
                                llm_provider="ai21",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"AI21Exception - {original_exception.message}",
                                llm_provider="ai21",
                                model=model,
                                request=original_exception.request,
                            )
                elif custom_llm_provider == "nlp_cloud":
                    if "detail" in error_str:
                        if "Input text length should not exceed" in error_str:
                            exception_mapping_worked = True
                            raise ContextWindowExceededError(
                                message=f"NLPCloudException - {error_str}",
                                model=model,
                                llm_provider="nlp_cloud",
                                response=getattr(original_exception, "response", None),
                            )
                        elif "value is not a valid" in error_str:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"NLPCloudException - {error_str}",
                                model=model,
                                llm_provider="nlp_cloud",
                                response=getattr(original_exception, "response", None),
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=500,
                                message=f"NLPCloudException - {error_str}",
                                model=model,
                                llm_provider="nlp_cloud",
                                request=original_exception.request,
                            )
                    if hasattr(
                        original_exception, "status_code"
                    ):  # https://docs.nlpcloud.com/?shell#errors
                        if (
                            original_exception.status_code == 400
                            or original_exception.status_code == 406
                            or original_exception.status_code == 413
                            or original_exception.status_code == 422
                        ):
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"NLPCloudException - {original_exception.message}",
                                llm_provider="nlp_cloud",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif (
                            original_exception.status_code == 401
                            or original_exception.status_code == 403
                        ):
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"NLPCloudException - {original_exception.message}",
                                llm_provider="nlp_cloud",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif (
                            original_exception.status_code == 522
                            or original_exception.status_code == 524
                        ):
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"NLPCloudException - {original_exception.message}",
                                model=model,
                                llm_provider="nlp_cloud",
                            )
                        elif (
                            original_exception.status_code == 429
                            or original_exception.status_code == 402
                        ):
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"NLPCloudException - {original_exception.message}",
                                llm_provider="nlp_cloud",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif (
                            original_exception.status_code == 500
                            or original_exception.status_code == 503
                        ):
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"NLPCloudException - {original_exception.message}",
                                llm_provider="nlp_cloud",
                                model=model,
                                request=original_exception.request,
                            )
                        elif (
                            original_exception.status_code == 504
                            or original_exception.status_code == 520
                        ):
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"NLPCloudException - {original_exception.message}",
                                model=model,
                                llm_provider="nlp_cloud",
                                response=getattr(original_exception, "response", None),
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"NLPCloudException - {original_exception.message}",
                                llm_provider="nlp_cloud",
                                model=model,
                                request=original_exception.request,
                            )
                elif custom_llm_provider == "together_ai":
                    try:
                        error_response = json.loads(error_str)
                    except Exception:
                        error_response = {"error": error_str}
                    if (
                        "error" in error_response
                        and "`inputs` tokens + `max_new_tokens` must be <="
                        in error_response["error"]
                    ):
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"TogetherAIException - {error_response['error']}",
                            model=model,
                            llm_provider="together_ai",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "error" in error_response
                        and "invalid private key" in error_response["error"]
                    ):
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"TogetherAIException - {error_response['error']}",
                            llm_provider="together_ai",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "error" in error_response
                        and "INVALID_ARGUMENT" in error_response["error"]
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"TogetherAIException - {error_response['error']}",
                            model=model,
                            llm_provider="together_ai",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "A timeout occurred" in error_str:
                        exception_mapping_worked = True
                        raise Timeout(
                            message=f"TogetherAIException - {error_str}",
                            model=model,
                            llm_provider="together_ai",
                        )
                    elif (
                        "error" in error_response
                        and "API key doesn't match expected format."
                        in error_response["error"]
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"TogetherAIException - {error_response['error']}",
                            model=model,
                            llm_provider="together_ai",
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "error_type" in error_response
                        and error_response["error_type"] == "validation"
                    ):
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"TogetherAIException - {error_response['error']}",
                            model=model,
                            llm_provider="together_ai",
                            response=getattr(original_exception, "response", None),
                        )
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"TogetherAIException - {original_exception.message}",
                                model=model,
                                llm_provider="together_ai",
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"TogetherAIException - {error_response['error']}",
                                model=model,
                                llm_provider="together_ai",
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"TogetherAIException - {original_exception.message}",
                                llm_provider="together_ai",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 524:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"TogetherAIException - {original_exception.message}",
                                llm_provider="together_ai",
                                model=model,
                            )
                    else:
                        exception_mapping_worked = True
                        raise APIError(
                            status_code=original_exception.status_code,
                            message=f"TogetherAIException - {original_exception.message}",
                            llm_provider="together_ai",
                            model=model,
                            request=original_exception.request,
                        )
                elif custom_llm_provider == "aleph_alpha":
                    if (
                        "This is longer than the model's maximum context length"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"AlephAlphaException - {original_exception.message}",
                            llm_provider="aleph_alpha",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "InvalidToken" in error_str or "No token provided" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"AlephAlphaException - {original_exception.message}",
                            llm_provider="aleph_alpha",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif hasattr(original_exception, "status_code"):
                        verbose_logger.debug(
                            f"status code: {original_exception.status_code}"
                        )
                        if original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AlephAlphaException - {original_exception.message}",
                                llm_provider="aleph_alpha",
                                model=model,
                            )
                        elif original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AlephAlphaException - {original_exception.message}",
                                llm_provider="aleph_alpha",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"AlephAlphaException - {original_exception.message}",
                                llm_provider="aleph_alpha",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 500:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"AlephAlphaException - {original_exception.message}",
                                llm_provider="aleph_alpha",
                                model=model,
                                response=getattr(original_exception, "response", None),
                            )
                        raise original_exception
                    raise original_exception
                elif (
                    custom_llm_provider == "ollama" or custom_llm_provider == "ollama_chat"
                ):
                    if isinstance(original_exception, dict):
                        error_str = original_exception.get("error", "")
                    else:
                        error_str = str(original_exception)
                    if "no such file or directory" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"OllamaException: Invalid Model/Model not loaded - {original_exception}",
                            model=model,
                            llm_provider="ollama",
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Failed to establish a new connection" in error_str:
                        exception_mapping_worked = True
                        raise ServiceUnavailableError(
                            message=f"OllamaException: {original_exception}",
                            llm_provider="ollama",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Invalid response object from API" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"OllamaException: {original_exception}",
                            llm_provider="ollama",
                            model=model,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Read timed out" in error_str:
                        exception_mapping_worked = True
                        raise Timeout(
                            message=f"OllamaException: {original_exception}",
                            llm_provider="ollama",
                            model=model,
                        )
                elif custom_llm_provider == "vllm":
                    if hasattr(original_exception, "status_code"):
                        if original_exception.status_code == 0:
                            exception_mapping_worked = True
                            raise APIConnectionError(
                                message=f"VLLMException - {original_exception.message}",
                                llm_provider="vllm",
                                model=model,
                                request=original_exception.request,
                            )
                elif custom_llm_provider == "azure" or custom_llm_provider == "azure_text":
                    message = get_error_message(error_obj=original_exception)
                    if message is None:
                        if hasattr(original_exception, "message"):
                            message = original_exception.message
                        else:
                            message = str(original_exception)
    
                    if "Internal server error" in error_str:
                        exception_mapping_worked = True
                        raise litellm.InternalServerError(
                            message=f"AzureException Internal server error - {message}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "This model's maximum context length is" in error_str:
                        exception_mapping_worked = True
                        raise ContextWindowExceededError(
                            message=f"AzureException ContextWindowExceededError - {message}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "DeploymentNotFound" in error_str:
                        exception_mapping_worked = True
                        raise NotFoundError(
                            message=f"AzureException NotFoundError - {message}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        (
                            "invalid_request_error" in error_str
                            and "content_policy_violation" in error_str
                        )
                        or (
                            "The response was filtered due to the prompt triggering Azure OpenAI's content management"
                            in error_str
                        )
                        or "Your task failed as a result of our safety system" in error_str
                        or "The model produced invalid content" in error_str
                        or "content_filter_policy" in error_str
                    ):
                        exception_mapping_worked = True
                        raise ContentPolicyViolationError(
                            message=f"litellm.ContentPolicyViolationError: AzureException - {message}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "invalid_request_error" in error_str:
                        exception_mapping_worked = True
                        raise BadRequestError(
                            message=f"AzureException BadRequestError - {message}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif (
                        "The api_key client option must be set either by passing api_key to the client or by setting"
                        in error_str
                    ):
                        exception_mapping_worked = True
                        raise AuthenticationError(
                            message=f"{exception_provider} AuthenticationError - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            litellm_debug_info=extra_information,
                            response=getattr(original_exception, "response", None),
                        )
                    elif "Connection error" in error_str:
                        exception_mapping_worked = True
                        raise APIConnectionError(
                            message=f"{exception_provider} APIConnectionError - {message}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            litellm_debug_info=extra_information,
                        )
                    elif hasattr(original_exception, "status_code"):
                        exception_mapping_worked = True
                        if original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AzureException - {message}",
                                llm_provider="azure",
                                model=model,
                                litellm_debug_info=extra_information,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AzureException AuthenticationError - {message}",
                                llm_provider="azure",
                                model=model,
                                litellm_debug_info=extra_information,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"AzureException Timeout - {message}",
                                model=model,
                                litellm_debug_info=extra_information,
                                llm_provider="azure",
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"AzureException BadRequestError - {message}",
                                model=model,
                                llm_provider="azure",
                                litellm_debug_info=extra_information,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"AzureException RateLimitError - {message}",
                                model=model,
                                llm_provider="azure",
                                litellm_debug_info=extra_information,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"AzureException ServiceUnavailableError - {message}",
                                model=model,
                                llm_provider="azure",
                                litellm_debug_info=extra_information,
                                response=getattr(original_exception, "response", None),
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"AzureException Timeout - {message}",
                                model=model,
                                litellm_debug_info=extra_information,
                                llm_provider="azure",
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"AzureException APIError - {message}",
                                llm_provider="azure",
                                litellm_debug_info=extra_information,
                                model=model,
                                request=httpx.Request(
                                    method="POST", url="https://openai.com/"
                                ),
                            )
                    else:
                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors
                        raise APIConnectionError(
                            message=f"{exception_provider} APIConnectionError - {message}\n{traceback.format_exc()}",
                            llm_provider="azure",
                            model=model,
                            litellm_debug_info=extra_information,
                            request=httpx.Request(method="POST", url="https://openai.com/"),
                        )
                if custom_llm_provider == "openrouter":
                    if hasattr(original_exception, "status_code"):
                        exception_mapping_worked = True
                        if original_exception.status_code == 400:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"{exception_provider} - {error_str}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 401:
                            exception_mapping_worked = True
                            raise AuthenticationError(
                                message=f"AuthenticationError: {exception_provider} - {error_str}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 404:
                            exception_mapping_worked = True
                            raise NotFoundError(
                                message=f"NotFoundError: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 408:
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"Timeout Error: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 422:
                            exception_mapping_worked = True
                            raise BadRequestError(
                                message=f"BadRequestError: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 429:
                            exception_mapping_worked = True
                            raise RateLimitError(
                                message=f"RateLimitError: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 503:
                            exception_mapping_worked = True
                            raise ServiceUnavailableError(
                                message=f"ServiceUnavailableError: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                response=getattr(original_exception, "response", None),
                                litellm_debug_info=extra_information,
                            )
                        elif original_exception.status_code == 504:  # gateway timeout error
                            exception_mapping_worked = True
                            raise Timeout(
                                message=f"Timeout Error: {exception_provider} - {error_str}",
                                model=model,
                                llm_provider=custom_llm_provider,
                                litellm_debug_info=extra_information,
                            )
                        else:
                            exception_mapping_worked = True
                            raise APIError(
                                status_code=original_exception.status_code,
                                message=f"APIError: {exception_provider} - {error_str}",
                                llm_provider=custom_llm_provider,
                                model=model,
                                request=original_exception.request,
                                litellm_debug_info=extra_information,
                            )
                    else:
                        # if no status code then it is an APIConnectionError: https://github.com/openai/openai-python#handling-errors
                        raise APIConnectionError(
                            message=f"APIConnectionError: {exception_provider} - {error_str}",
                            llm_provider=custom_llm_provider,
                            model=model,
                            litellm_debug_info=extra_information,
                            request=httpx.Request(
                                method="POST", url="https://api.openai.com/v1/"
                            ),
                        )
            if (
                "BadRequestError.__init__() missing 1 required positional argument: 'param'"
                in str(original_exception)
            ):  # deal with edge-case invalid request error bug in openai-python sdk
                exception_mapping_worked = True
                raise BadRequestError(
                    message=f"{exception_provider} BadRequestError : This can happen due to missing AZURE_API_VERSION: {str(original_exception)}",
                    model=model,
                    llm_provider=custom_llm_provider,
                    response=getattr(original_exception, "response", None),
                )
            else:  # ensure generic errors always return APIConnectionError=
                """
                For unmapped exceptions - raise the exception with traceback - https://github.com/BerriAI/litellm/issues/4201
                """
                exception_mapping_worked = True
                if hasattr(original_exception, "request"):
                    raise APIConnectionError(
                        message="{} - {}".format(exception_provider, error_str),
                        llm_provider=custom_llm_provider,
                        model=model,
                        request=original_exception.request,
                    )
                else:
>                   raise APIConnectionError(
                        message="{}\n{}".format(
                            str(original_exception), traceback.format_exc()
                        ),
                        llm_provider=custom_llm_provider,
                        model=model,
                        request=httpx.Request(
                            method="POST", url="https://api.openai.com/v1/"
                        ),  # stub the request
                    )
E                   litellm.exceptions.APIConnectionError: litellm.APIConnectionError: transform_response not implemented. Done in watsonx/completion handler.py
E                   Traceback (most recent call last):
E                     File "/Users/krrishdholakia/Documents/litellm/litellm/main.py", line 2633, in completion
E                       response = base_llm_http_handler.completion(
E                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                     File "/Users/krrishdholakia/Documents/litellm/litellm/llms/custom_httpx/llm_http_handler.py", line 225, in completion
E                       return provider_config.transform_response(
E                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E                     File "/Users/krrishdholakia/Documents/litellm/litellm/llms/watsonx/completion/transformation.py", line 271, in transform_response
E                       raise NotImplementedError(
E                   NotImplementedError: transform_response not implemented. Done in watsonx/completion handler.py

../../litellm/litellm_core_utils/exception_mapping_utils.py:2122: APIConnectionError
---------------------------- Captured stdout setup -----------------------------
<module 'litellm' from '/Users/krrishdholakia/Documents/litellm/litellm/__init__.py'>

pytest fixture - resetting callbacks
----------------------------- Captured stdout call -----------------------------


[92mRequest to litellm:[0m
[92mlitellm.completion(model='watsonx_text/ibm/granite-13b-chat-v2', messages=[{'content': 'Write a short poem about the sky', 'role': 'user'}], stop=['stop'], max_tokens=20)[0m


SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
Final returned optional params: {'stop_sequences': ['stop'], 'max_new_tokens': 20}
[92m

POST Request Sent from LiteLLM:
curl -X POST \
https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-03-13 \
-H 'Content-Type: *****' -H 'Accept: *****' -H 'Authorization: Bearer eyJraWQiOiIyMDI0MTIwMTA4NDMiLCJhbGciOiJSUzI1NiJ9.eyJpYW1faWQiOiJJQk1pZC02OTgwMDBENzZUIiwiaWQiOiJJQk1pZC02OTgwMDBENzZUIiwicmVhbG1pZCI6IklCTWlkIiwianRpIjoiZDUwODhjZjQtZTFhNS00ZTg3LWE0NzUtZWE1MDgwNzQ1NzU3IiwiaWRlbnRpZmllciI6IjY5ODAwMEQ3NlQiLCJnaXZlbl9uYW1lIjoiS3Jpc2ggQW1pdCIsImZhbWlseV9uYW1lIjoiRGhvbGFraWEiLCJuYW1lIjoiS3Jpc2ggQW1pdCBEaG9sYWtpYSIsImVtYWlsIjoia3JyaXNoZGhvbGFraWFAZ21haWwuY29tIiwic3ViIjoia3JyaXNoZGhvbGFraWFAZ21haWwuY29tIiwiYXV0aG4iOnsic3ViIjoia3JyaXNoZGhvbGFraWFAZ21haWwuY29tIiwiaWFtX2lkIjoiSUJNaWQtNjk4MDAwRDc2VCIsIm5hbWUiOiJLcmlzaCBBbWl0IERob2xha2lhIiwiZ2l2ZW5fbmFtZSI6IktyaXNoIEFtaXQiLCJmYW1pbHlfbmFtZSI6IkRob2xha2lhIiwiZW1haWwiOiJrcnJpc2hkaG9sYWtpYUBnbWFpbC5jb20ifSwiYWNjb3VudCI6eyJ2YWxpZCI6dHJ1ZSwiYnNzIjoiMDYxODA5M2RkMDBlNDI1NWI2MWY4M2UzOTJkMmExY2IiLCJpbXNfdXNlcl9pZCI6IjEyOTA2NjcwIiwiZnJvemVuIjp0cnVlLCJpbXMiOiIyOTYyNTUwIn0sImlhdCI6MTczNDgwNzIxMywiZXhwIjoxNzM0ODEwODEzLCJpc3MiOiJodHRwczovL2lhbS5jbG91ZC5pYm0uY29tL2lkZW50aXR5IiwiZ3JhbnRfdHlwZSI6InVybjppYm06cGFyYW1zOm9hdXRoOmdyYW50LXR5cGU6YXBpa2V5Iiwic2NvcGUiOiJpYm0gb3BlbmlkIiwiY2xpZW50X2lkIjoiZGVmYXVsdCIsImFjciI6MSwiYW1yIjpbInB3ZCJdfQ.a4tZ4OoiieznS4AHOc9DsiaABSvEWj6fn0d6iLOXoQ7aR07fRwC82gXLpfns5i3ZF2dtYvNns-IbE_0lGIvKvvdYtdPNYxDyL0GWxqguUF7_tmRLRoBsCgPLcKDcvmAENk9m99YYJjsudSvp_jPTWHOLlpB2M7_3TbiHHS9Vl2MYaGLCLTwvFUDDg0gCuAxOi88TfSCOUSIucT_QYlf9AhmUAhfhHA4aXfY0dkCsqzsXq6IquTrsDQrMwtN9peMr0QR0Gqh7bwGperMaZ4jB0l7zV5quIqw1PFzouiVqbB********************************************' \
-d '{'input': '<|user|>\nWrite a short poem about the sky\n<|assistant|>', 'moderations': {}, 'parameters': {'stop_sequences': ['stop'], 'max_new_tokens': 20}, 'model_id': 'ibm/granite-13b-chat-v2', 'project_id': '32a5f963-c78b-4ff8-b3d4-1e5538a14c99'}'
[0m


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

----------------------------- Captured stderr call -----------------------------
[92m10:53:36 - LiteLLM:WARNING[0m: utils.py:308 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
------------------------------ Captured log call -------------------------------
WARNING  LiteLLM:utils.py:308 `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.
=============================== warnings summary ===============================
../../litellm/utils.py:146
  /Users/krrishdholakia/Documents/litellm/litellm/utils.py:146: DeprecationWarning: open_text is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.
    with resources.open_text(

test_completion.py:2111
  /Users/krrishdholakia/Documents/litellm/tests/local_testing/test_completion.py:2111: PytestUnknownMarkWarning: Unknown pytest.mark.flaky - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.flaky(retries=3, delay=1)

test_completion.py:3917
  /Users/krrishdholakia/Documents/litellm/tests/local_testing/test_completion.py:3917: PytestUnknownMarkWarning: Unknown pytest.mark.flaky - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.flaky(retries=3, delay=1)

test_completion.py:4289
  /Users/krrishdholakia/Documents/litellm/tests/local_testing/test_completion.py:4289: PytestUnknownMarkWarning: Unknown pytest.mark.flaky - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.flaky(retries=3, delay=1)

test_completion.py:4318
  /Users/krrishdholakia/Documents/litellm/tests/local_testing/test_completion.py:4318: PytestUnknownMarkWarning: Unknown pytest.mark.flaky - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.flaky(retries=3, delay=1)

test_completion.py::test_completion_watsonx_error
  /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/httpx/_content.py:202: DeprecationWarning: Use 'content=<...>' to upload raw bytes/text content.
    warnings.warn(message, DeprecationWarning)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED test_completion.py::test_completion_watsonx_error - litellm.exceptions...
======================== 1 failed, 6 warnings in 2.60s =========================
