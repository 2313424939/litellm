general_settings:
  store_model_in_db: true
  database_connection_pool_limit: 20

model_list:
  - model_name: fake-openai-endpoint
    litellm_params:
      model: openai/my-fake-model
      api_key: my-fake-key
      api_base: https://exampleopenaiendpoint-production.up.railway.app/
router_settings:
  routing_strategy: "latency-based-routing"
  routing_strategy_args: {"ttl": 100} # Average the last 10 calls to compute avg latency per model
  allowed_fails: 1
  num_retries: 3
  retry_after: 5 # seconds to wait before retrying a failed request
  cooldown_time: 30 # seconds to cooldown a deployment after failure
  enable_pre_call_checks: true
litellm_settings:
  return_response_headers: true
  callbacks: ["prometheus"]
  cache: true
  cache_params:
    type: "redis"
